---
title: "On preprocessing EEG data"
source: "https://www.youtube.com/watch?v=NHKIPU_T7-0&t=953s"
author:
  - "[[Makoto Miyakoshi]]"
published: 2023-10-17
created: 2024-12-24
description: "A presentation at NIH HEALthy Brain and Child Development Study (HBCD) summer seminar. Recorded on July 23, 2023.https://heal.nih.gov/research/infants-and-children/healthy-brain"
tags:
  - "clippings"
---
# On preprocessing EEG data
# Notes
He is a very interesting speaker. He finds the most interesting arguments.
# Content 
Video title: On preprocessing EEG data
    
Video URL: https://www.youtube.com/watch?v=NHKIPU_T7-0&t=953s
    
Video language: English (auto-generated)
    
--------------------------------

[Music] great so hello everyone I want to welcome you to the second in our hbcd EEG uh lunch Le lecture series um for those of you on the East Coast um it's my pleasure to introduce uh today Dr Makoto makosi who is going to be uh talking to us about EEG processing and and uh Makoto said that he's willing to uh take questions during the talk so what's the best way for us to do that Makoto should people uh just raise their hands or or just say something to yeah say say something is okay so if you have a question uh Makoto is willing to entertain it during the talk um uh so please uh feel free to do that okay Makoto the floor is yours thanks uh let me share my screen let me see well let me see how does it work let me try this there you go okay yeah perfect oh good thanks um hello everybody I am Mako MOSI I am an assistant professor at Cincinnati Children's hospital today I'm going to talk about the pre-processing of EEG data um when we talk about the pre-processing of course the first question is why do we even need to do that and the answer is obvious that's because you know what we call EEG signal contains various other kinds of signals than you know pure brain signals um so let me start with this history because history is always fun um so this is hansberger the he is the first mankind who reported the sculp recorded like non-invasive EEG in [[1929]] but the interesting fact is that he he made the first observation of the EEG in [[1924]] so he spent five years wondering if he he should publish it or not that's precisely because he also knew that there are a lot of other kinds of the artifacts in his data so it was a dilemma for him you know he he knew the value of his potential findings but at the same time there is a huge risk of you know publishing something completely nonsensical so um in order to publish his first paper he conducted 174 sessions and he made 866 measurements which is a you know really large number right and at the point of 1928 which is one year before his first publication in his diary he says oh my gosh I totally wasted my time you I I shouldn't have done anything about this EG because this is completely bogus you know so he was really oscillating between this confidence confident State and you know losing the confidence at all so in in his first paper spending you know almost more than 20% of the volume he argued the various types of the artifacts which is listed here here and you know this concern is still valid for today's EG um research and the persistent skepticism about the brain origin n of the EEG continued more than we usually think you know this is a 1970 paper which is almost um 40 years after the barbar's first publication about the finding of the the noninvasive sculpt EEG uh this professor from UC um UC London I think um said that all this EEG Alpha what you call is actually not brain origin you know he said this this is actually from the eyes and this was the 7s and much later in like 1987 there was a the final definitive prove that EEG signal is from the brain of course at this time at you know by by this time there are a bunch of EEG papers so maybe it was not really necessary but you know um according to one of the the you know the big books I I've read this is the you know final proof so what they did was they used this animal dog and they isolated the brain but not like physically isolated I I think still in Vivo situation but you know maybe those some connections are cut um and in order to remove the eye artifact they removed the eyes and in order to remove the heart ARA they stopped the heart and they even used this artificial heart to you know eliminate the electric signals from the heart but still maintaining this um perfusion so that when they turn on the machine machine the you know the blood circulates and the E it's generated and when they turned off the machine then of course blood circulation stops then that you know makes the EEG level much lower but this on and off the heart is something barer couldn't do uh when when when he was doing his time but after you know making those experiment we can finally say that yes signal is really from the brain um but um as I said you it does not necessarily guarantee the optimism uh we may enjoy for example this is a 2007 paper and um what they did in this study is that U they um used a special type of the the um the anesthesia only that works only for this um upper body so the the subject loses the controls and muscle in the face and probably the the large part of the upper limbs but the point is subject can still uh use the fingers to make responses and subject is also fully awake and this um paralysis does not affect the subject um con um Consciousness and the cognitive abilities so subject basically performs the cognitive task while only using this fingertip and everything else in the upper body at least is you know completely paralyzed um so this with paralysis I mean the Ania they recorded the power Spectrum density that looks like B Trace B There are four lines in B because there are four types of the cognitive tasks but you know these task differences um don't matter at this point and this Trace is a this is without the anesthesia so what we can tell from here is that um you know um we think we can analyze beta gamma range but um this is kind of the grand truth that how much muscle contribution it's still there when we analyze beta and gamma and in the right plot um you see the the exponentially Rising traces um and this is basically the difference between with and without using the the anesthesia when doing the same task and so basically this is the the the amount of the contribution by muscle right and it's exponentially increasing towards the higher frequencies um so it's very obvious that without without the mus muscle contribution the one over fness of the EEG power Spectrum density is very clear right and with the muscle contribution it kind of Lees to the floor which is rather flat towards the higher frequencies um so you know this this is uh always happens and everybody's you know our data contains this muscle contribution all the time so that we have to remember all the time right another um um paper I like to site here in this context is this um yal Greenberg 2007 this this paper was published from neuron which is a very high you know profile Journal um so the the context of this paper was that at that time you know everybody I mean the EEG researchers are very um interested in performing the gam band analysis because there are interesting um theories like or gamma band is um must be related to The Binding like brains binding function and stuff like that but this ubal Greenberg showed that this gamma oscillation is basically um sard this is from the eyes remember this guy from the 1970s that professor said no no this is from eyes which was refuted later but ubal Greenberg he showed that you know um probably most of the cognitive you know laboratory experiment using humans um if you see something like this so this looks very much like oh you know this gamma band oscillation seems to be related to how this car image is chopped up you know if this is flipped then you know even stronger gamma so it's you know suggest as if this is a cognitive process but actually he he showed that um when he used this I trer then this high frequency oscillation was observed at the moment of the sard and when he separated the trials into with and without sakars then you know this gamma band activity disappeared when there was no sard so this kind of um shocking results um periodically appears in our field so we have to always remember um that you know there was this case there was that case because that is only um effective counter measure we can take you know so we learn we should learn from history really um this is much less established but could be of interest in in this context now the left hand panel you see the the recent Nature Neuroscience publication from the UCSD um the the pi the BR Boch was sitting in upstairs of my office when I was in the UCSD last year what they did was um you know they developed some interesting method to decompose the power Spectrum density into this exponential curve and and basically the alpha Peak so you fit the curve like this and you subtract this exponent and you got the flattened um Power Spectrum density with big um Alpha Peak which you know you can also fit the gaussian um distribution so this um approach um makes an assumption that you know this exponential one / f curve can be fit right and when I discussed this paper with one of my mentors in San Diego U Paul Nunes he said something quite unexpected he said it doesn't have to be um one over F and actually he does not believe that um this power Spectrum density contains one over F at least not all the time and I was so puzzled and I asked him why and he he brought his book and he showed me those figures and he said look do you think these are following one over F and we have to pay close attention to the unit okay so here um the Vex paper uses log scale so you know you see this curve uh kind of more exaggerated and in this plot these plot it's um it's a square root of the uh the power so this is basically the the microvolt and not the microvolt Square so this exponent is less um exaggerated here um so if you convert it to the log scale then this is even flatter you so it's much less flatter uh than these plots and I asked him okay I I see your data is not following the one over F but what what do you think is the reason for this and his explanation was very shocking to me he said because when I record did these data I watched all the subjects every subject very closely I I watched them and even if they make the slightest body movement I said don't move and he recorded the data in that way so he knew that the data he recorded is completely you know as ideal as possible you know super perfectionistic those data were each data point were recorded from the subjects and he said um probably you me are using the data recorded in hospital and sometimes yeah it's from hospital but you know many times because I'm a psychologist you my my data sets were from the cognitive experiment Labs like psychologist labs and you know you don't watch those data collection very closely that's why you miss the body movement and that's you know apparently this body movement contributes to the lower frequencies and which is probably why you see a nice one over F car um so you know this kind of you know how much we care for the artifact really is a fundamental issue so that's why um and I I talk about these things first because these artifact may or may not be treated in the later signal processing so there are things we can do and we cannot do in signal processings right so someone said go garbage in garbage out so but you know when when we take this garbage first then we might even take it into you know the basis of how we see the data for example one / f in the power Spectrum density is the is the natural property of the E EG signal but is that really so you know this is a really fundamental question right so we have to remember this now let me uh quickly go to the next step which is uh what we can do pre-processing so this is typically how we see the data when when we see this you know High amplitude bars in our time series presentations and we think oh this is the you know this this part needs to be removed like we see it as if a Rotten you know damaged part of an Apple so you know that can be cut out to clean the data so this is like the step one of data pre-processing and usually you know this this method is effective enough but here you know we have to also remember this fundamental fact so let me talk about it um in upper row I I see how the ground truth of the MRI signal is uh collected and relates to the ground truth and its representation in data so let's say this um uh Phantom we call the Phantom is defined in this in this triangle and we scan it in MRI scanner and in in your console or ma lab you know you you see this reconstructed Grand truth but because we know that this reconstructed Grand truth corresponds to this grant truth we used um when we find something you know a figure here then we can safely assume that okay there was something also in this ground truth like chipping or something right so this ground trth correspondence in the case of MRI is very clear it's very straightforward you can also perform the Mr um microscopy which means if you increase the the resolution of the MRI like 0.1 millimeter or something you can still get the image of course it takes time to you know take the scan but you can you can do this can we do the EEG um microscopy no there is um there is a special you know night quest of the special resolution of EG and stuff so the fundamental question I mean the problem of the EEG is that there is no such a thing as you know the grand truth of EG which is you know experimentally available of course there is um the grand truth of the E which is the activation of those neurons and snapsies and stuff but um those are distributed and you know um and it's regulated in a very complicated chaotic way um and uh so we cannot assume the simple linear um assumption between Grand truth and the recorded signal so I used to say that oh because our grand truth in EG is so um complicated and inscrutable we generate as colorful figures as possible to escape from the fact that we don't have um clear ground truth so this is probably um you know our effort to escape from the grand trist you know we we try not to remember the grand trist problem um so this is also um very fundamental limitation of the which we have to always remember okay um I'll skip this actually uh so um here is um stratic question of okay when we when we clean the EG then we have to know what the what the clean EEG is right but can we mathematically or engineering wise you know define the properties of the clean e so there are some of these established criteria um for example the stationarity is a good concept stationerity it's as explanation um technical definition is could be uh pretty complicated but you know simple summary is here the mean and variance do not change over time so you set the arbitrary you know width of window and using that window you sample the data from the arbitrary you know time point and you take the mean and variance of those sample window and if those mean and variances are the same across all random sampled windows then you can say that your your data is a stationary know this is a like a simple version of the the definition this is also from the Wikipedia uh figures and you know one the the above U plot shows the example of the stationary data and the data shown in plot below is the example of the non-stationary time series data so that that is a bad signal and we can you know as an EEG researcher we can agree with this um good bad definitions right because in a lower plot we we we would say oh this looks like a you know bar right shaft potential that develops over like two seconds so we we want to apply a stronger hypers Spirit or something like that intuitively we can tell that right um but here here is the definition issue okay um when we look into the the Erp let's say starting from 200 millisecond before and 800 millisecond after the stimulus onset now is this 1 second Erp stationary or not and the answer is surprisingly oh this this is non stationary right because it's starts from the Baseline period which is quiet and you know centered at zero and you know and of course it shows the the brain response to the stimulus or whatever this behavioral output and you see P1 N1 P2 and2 P3 you know they def in latencies and frequencies um so single Erp is nonstationary which is a bit confusing but another confusing thing is that but if you repeat the this single trial for let's say 100 times then we can probably say that this data is overall stationary again because you see this repeating patterns right so if you take those random window sampling approach then you can see okay this part and this part are more or less the same more or less the same so the definition of the stationarity actually depends on the time scale right so that's why it's a bit confusing it's not like or averaging should be done in this way you know no one no one makes mistake in taking the average because it's so simple and understandable but the definition of the stationarity you know it's it's like okay stationarity in what's what time scale are you talking about minute second or hour or millisecond know so this is why you know this concept of the stationarity is not very popular and you know not very much used in explicit way in calculation but stationarity is still a very very very important assumption and the concept in pre-processing EG um I'm going to skip this okay um the second point is the frequency domain one over fness so we have already seen a bit of this but let me go into that so let me give you a quiz here okay I'm going to show you two time Series E data this is a this is b a b my question for you is which um time series looks more like EG is it a or b and I would say you know by the way this is a five seconds okay the same five seconds I would say a contains a lot of high frequency noise because you know so much Spike and also the lines seems really flat maybe too strong Hy was applied or something but you know this looks like too noisy to me visually B yeah this is like um the balance between this low frequency wiggling and high frequency spikes are more balanced although we don't we don't see the cross Channel correlation in either of the cases A and B that's because these These are simulated data anyways so um the those differences we saw when we compared A and B can be best um explained by this uh Power Spectrum density plot so a is a White Noise White Noise is by definition you know it contains energy the same amount of energy across all the frequency ranges so it's kind of funny to say that that you know EG Delta and Alpha has the power and Gamma has the same amount of the power you know that's like a flat so if if there were such an EEG data then that should look like the plot a but almost always you know what we see the the genuine EEG data it it it's closer to the pink noise which is which has this one over f c so um this Brad Vortex lab developed the method called fof fitting oscillation and one over f f um in 2020 and this is not only a tool like a convenient tool to extract EEG features but it has a pretty interesting physiological um assumption and usefulness in in the Neuroscience so let's say there are excitatory and inhibitory input system in for the neuron and excitatory system is um controlled by ampa input and inhibitory is controlled by gva and now first we see the time domain this Spike response of this ampa and Gaba and it's obvious that ampa has a sharper Spike right and Gava has a broader Spike sharper the spike uh when you convert it to the frequency domain um um the complete Spike has a uniform distribution across the frequency be so sharper Spike means flatter and that's the case of the ampa and Gaba you know it has a broader Spike broader means it has a low frequency you know more than high frequencies so um the power Spectrum density should show the more more expon and you know it's it's very interesting they said um therefore when we see a flatter power Spectrum density in our EEG data that is probably dominated by ampa and if we see the exponent that in in our EG power Spectrum density then that may be dominated by Gaba and Amper dominated Gaba dominated imediator means you know excitatory or inhibitory right this this is very interesting argument and I I like this Simplicity a lot um so um we can take advantage of these um known facts or at least proposed facts and um so you know broad Vortex point was the difference between this exponent but at least we can say that whether it's ex you know the excitatory or inhibitory it follows more or less one over F right and and if if we see the power Spectrum density goes against such a pattern so for example um this this cluster five well this is actually okay but cluster six this looks like a square root sign and it doesn't follow the one / f then we can simply say oh because it's not the power Spectrum density does not follow the one / f pattern um we declare that this is not representing the pure brain signal you know it's very simple and um convenient criteria we can use um maybe I'm going to skip this also so the section summary um one over fness is always available and it's pretty reliable I think although we have to remember the the skepticism by paas maybe the low lowest end of the power Spectrum density is highly contaminated by body movement and of course the high part like the gamma part is contaminated by the muscle as well uh the final thing we can talk about is you know uh when we talk about the signal process like pre-processing we can always follow the established methods so which may or may not require the the declarative knowledge of anatomy or you know the neuroscience per say so again we're back to this figure and now let's do something we can do um the first choice would be to select this window and what we called reject so that means um you select this window and reject and you Stitch the before and after this rejection so there is um discontinuity there right after removing this window and not many people people talk about this effect of the discontinuity so I run some simulation for this presentation and top row this is the the intact signal like simulated signal I generated for for this this is a four Hearts oscillation for 30 seconds and in the top row you see the time domain uh representation and in in the bottom you see the time frequency domain now um I rejected let's say 15 windows out of that wind um The Continuous signal and that introduced the 30 oh sorry 30 30 Windows to introduce 30 discontinuities um in order to make the fair comparison you know I kept the data length the same so you can imagine that this data was actually 40 second and I I rejected 10 seconds over it to make it 30 seconds so now you see this discontin nties here right and how do that affect on the time frequency representation and it looks like this and you can see sometimes this um the central power goes down below four Hearts that's because this McDonald like shape is more like um big you know one big oscillation with a lower frequency or sometimes uh this power goes up because um it this this discontinuity created a sharper spikes and um either this central power goes up or down every time this discontinuity happens you see this um you know um conical shape power goes up to the higher frequencies that is because this um the spike the very very fine Spike contains um a lot of the the harmonics and harmonics is um is the know collection of the higher frequencies so you see those um um the conical shape everywhere um what's so interesting to me is that sometimes you know depending on how this discontinuity is defined um you completely lose the power at all and you know this is pretty interesting to observe this so anyways um of course this standard like filter functions let's say in egab is designed so that they do not um um go over this discontinuities but every time this discontinuity is detected then this filter function stops before you know uh when the right Edge touches this uh discontinuity and it resume the process from the the you know left Edge touching from that discontinuity so that this discontinuity is not um processed included in the final result so from the user Viewpoint this because this this is you know this process is taken care of under the hood so maybe it's not very obvious but from the analyst like this you know processing Viewpoint introducing you know a lot of this discontinuity is uh very makes the the uh process very messy and also it um reduces some sensitivity of the analysis and of course the power Spectrum density comparison shows uh the effect of this uh discontinuities so this time window rejection although it seems simple and cannot go wrong um it can actually go wrong depending on what kind of analysis you want to do after so that's why um and also it's you know it requires your your eyes and hands to go over so that's a problem so uh you know there is a signal processing I can recommend to perform this window rejection so um this is another simulation data but this is the real data published uh 20 some some 20 years ago so this is the EEG lab's tutorial data 32 channels and some subject performing the visual attention task and there are 80 Trials of 3 second epox okay on the left hand side I'm showing the original data so this is a beautiful Erp from the 32 channels butterfly plot on the right hand side you see a whole bunch of uh positive spikes everywhere between 200 to 800 millisecond range that is added in this way so in this simulation you know for every trial okay every Epoch I um added 2,000 microvolt um Spike um in randomly selected Channel and randomly selected latency between minus 200 to positive 800 millisecond the whole point of doing this is that um when you perform the standard Epoch rejection procedure after adding this artifact in this way the the remaining number of you know the usable remaining trial is zero of course that's because every aoch contains at least one you know 2,000 microvolt spike in one of the channels right so you cannot escape from that then um here is our solution um so this solution is called ASR artifact Subspace reconstruction and you see 8 and 12 and 14 and 20 these are the threshold so 20 is the most gentle here and SR8 is most stringent and aggressive and you see this spikes nicely start to disappear and maybe but in asr2 you still see the spike here so maybe the balancing point is either ASR eight or 10 but if you if you pay close attention to this peak amplitude um you know this when you use ASR 8 it's probably less than 30 although when you use the gentle correction you you have something like 35ish so you you clean those data I mean artifact at the cost of the genuine signal of course this is a trade-off like that so um you can also use a hybrid approach which is asr2 which is the most gentle um correction together with the epoch rejection then you don't lose the amplitude very much you still maintain 35ish Peak amplitude and you know know um of course you do uh number of the trials so you your your signal to noise ratio is um a bit lower but you can still continue the your research right so this is very um good solution and you know much better than nothing um there is um I can explain this technical um aspect of the ASR but I'm not going to go into that now because I'm running out of time but one thing I want to mention here is that even though ASR is known to be you know um like dependent on principal component analysis it's not the simple sliding window PCA rejection because PCA is a part of ASR which is you know y equals to V transpose X here but the the final um The Mixing Matrix is here which is act actually um they learned from the clean part of the data itself so it's not um PCA per se but it's um it's a mixture of PCA with the help of the clean part of the input data itself so that's why it's kind of smart and if you see this um contaminated data by your eye it's kind of you know you know what you want to do which is I want to break off this spine and you you know I want to do some minimal makeup for that pole and that's it right that's what we need that that's the minimum um um invasive approach but we know it doesn't hard because we know this the the duration of this spike is only like one frame like four millisecond so it shouldn't matter right and what ASR does is very close to what you're thinking now it what it does is you know it uses very narrow sliding window of half a second and it performs the channel by you know channelwise interpolation and of course this interpolation pattern differ from window to window so it has that flexibility and again as I said um ASR also uses the Learned data which is learned from the clean part of the data itself so it's when it performs the in inter interpolation it's not only using this surrounding channels data but also um surrounding Channel data is rotated in a special way so that it the the the result resemble the clean part of the the input data itself so it has such um um flexibility and adaptivity to each input data like subject by like individualized interpolation let's say um I'll skip that okay so that's a section summary is that um this continuous data cleaning method called ASR could be an alternative to the conventional window rejection and Cincinnati Children's Hospital has been working on this project um to see to evaluate whether we can automatize this EG cleaning process or not basically using these techniques okay the final conclusion is that uh is most reliable in mid-range which is Theta Alpha and probably low Beta And when we talk about you know very low frequency range as Delta or very high frequency range as gamma then we have to remember those skepticisms and um you know the the past publication that shocked all the EEG researchers um you know all this was not from the brain U those cre decision are always valid and we have to be careful um and it's also um a good thing to remember the the definition of the stationarity and uh this is one although this is not like unitary definition and there is no only one method to evaluate the stationarity but you know there are multiple methods to evaluate the stationarity but stationarity is a good concept to stick to when we talk about processing and one over fness again with the you know um the skepticism by Paul nunas um generally speaking if the power Spectrum density goes against the is this one over F then that's absolutely not from the brain so those empirical criteria are always usable useful and finally the ASR this kind of complicated signal processing uh may be uh intimidating because it you know it could it could look like a complete blockbox but um you don't need to understand the the every mechanism of those black boxes for example you can use a smartphone although you you don't know how it works right so what what we need to do is to understand the input output relation and we also have a control so you know where those knobs are to tweak so if you find over correction then how how can you reduce the amount of the correction or if if you see the under correction you want to increase the correction power you know as long as you have those controls over your data then I think that's fine so those are the pre-processing um um my my presentation okay thank you very much thanks M thank you do you w to um stop sharing and we can see if there are any questions sure great we do have questions hi thank you so much it was such a fantastic presentation I love that you showed all the history of this I think it's so valuable for grounding us I really really appreciate that um I had a question about your um ASR and in particular I'm wondering if the success of the method is going to depend on still somehow figuring out the right parameters for what you're detailing as clean data so you showed us a simulation where it was obvious what was clean and what was not that problem of defining clean and not clean gets harder in real data and even harder with threeyear olds twoy olds one year-olds so could you say a little bit more about how you are are defining clean data which ASR then smartly considers in its in its reconstruction yeah that's a good question so um we can I can use this self- referential way to explain how it works so I I said in my presentation the stationarity the definition of the stationarity is um convenient and also important um concept for the pre-processing the EG data right so ASR has a function like one of the internal functions of the ASR is to evaluate those sensity parameters um so because ASR cannot use our fistic decisions you know sometimes it's so easy for us to tell oh this is good this is bad but you know in order to replicate our human intuition with the those parameters then you know things start to appear very cranky um but ASR has um well first of all ASR is not a simple simple mechanism but it's a it's a really huge machinery and you have total of like 31 parameters you can tune so it's very uh and those parameter corresponds to you know the mathematical concept such as the gaussian distribution and trunked gaussian distribution and standard division those mathematical and statistical concepts are used um and also the key um key aspect of the ASR the the one of the sources of its robustness is uh that it uses the median statistics instead of the mean because mean is very weak against the outliers right but in the case of the median you can have up to like 50% of your data filled with the filled with the artifact and you can still perform the pretty standard analysis um so if you ask me how reliable ASR is when you have this infant data and you know this clinical data sets um it depends on um you know whether this input data contains a genuine uh EG signal and for example if I find okay okay this input data contains no EEG data at all then we cannot do anything and typically in the case of the ASR the definition of the good data is uh not an outliers in the sense of those median statistics okay um of course it can kick out is flat data as well because you know the the variance is too low so it it targets the good part of this data distribution and if if you go the extremity of this data distribution of course you see uh you know flat lines here and 2,000 microvolt over there so first you build this data distribution and you look into the the you know this central part which which cannot be um wrong at least in the sense of the extreme amplitude outliers so ASR is actually probably only only good at kicking out the high amplitude outliers and it doesn't do anything fancier than that it doesn't do anything in frequency domain it doesn't do anything about you know cross frequency something it doesn't do anything complicated it's actually very close to what we do when we perform the window rejection you know when we do window rejection usually in high amplitude is bad right that's the only principle we use and um well we can actually evaluate the frequency component a little bit but as we did in AB comparison in my demonstration but know usually High amplitude is bad and that's it so ASR is um out out liar rejector in the flexible and Smart Way Elizabeth hi makota thank you for such a great H talk that was really helpful I learned a lot and um I have a a little bit of a piggyback on Lindsay's question and then a real um practical question uh I see that we have a lot of grad students and trainees uh EEG Tex on the call today which is wonderful and so I wondered if you could take kind of a step back and talk about the particular kind of Developmental concerns and differences that we may encounter in our hbcd data for the very young kids and how the principles that you talked about relate to that and then second um for the um kind of implementation of this that you're talking about um with the stationerity is this implemented in EEG lab that's the real kind of nitty-gritty how do we how do we put this into practice with our own data thanks okay your your first question um particular things about the developmental data um you know I um moved to this Children's Hospital just a half a year ago and I'm not that U knowledgeable enough to talk about the developmental data this particular aspect of it but one thing I can say is that um uh the you know those child developmental studies um often those researchers use the egi recording system and um there is an egi issue I would say the data quality using egi even if you record the adults data using egi the data quality is not very impressive in my opinion um I haven't run any detailed um test yet but um um so there are two problems actually you are using those developmental population and you are using egi those two things are often kobed so it's kind of hard to separate for me I can't tell which is the problem can you um um so there's nothing we can do about uh working with developmental populations that's what the focus of our work is starting with very young infants but I'd like you to spend just a few minutes saying what issues you think are problematical with egi and we won't we won't we won't uh we won't uh let egi know what you're they don't have enough funds for for a hit team just yet so yeah right exactly right um one this um Hardware engineer told me that um egi does not have a d RL electrod which is a direct right leg electrode and usually those modern systems have those DRL El and this DRL is like um um it's like um another ler of the you know the differential yes um recording system for the ground potential yes that's right does not you're correct that it does not right and according to his explanation that's the reason so and I have couple of data um that kind of supports that view but I'm not myself I'm Hardware engineer so I I can't say for sure I do like to know as well but um can all right thank you uh Caitlyn thanks makota this was fantastic um I've been part of some conversations lately talking about the reasons for missing this as potentially being important to know um so thinking about um and and again sorry I I'm very much in a clinical population um so uh in developmental population but we've been talking a lot about how kind of characterizing the reasons for kind of data rejection could even be really helpful and so I'm curious your thoughts on um how well um and I I I'm in the process of moving more into EEG lab but I'm curious about kind of how the the pipeline that kind of accommodates or accounts for the reason reasons for missingness um and if you think um kind of just personally if it would be more helpful for us in our papers to be characterizing this so thinking of like should we be moving the field into having these conversations and and being more explicit about um the data cleaning that we do and what it ended up removing um okay uh I don't know if this answers to your question or not but um my sense how I feel is that um at some point you know you w to um so okay so there are two processes actually when we do the EG research one is to educate ourselves so we have to expose ourselves to the raw data in the time series and we have to go through you know thousands tens of thousands of you know those Epoch rejection so that we part become the you know artificial machine learning device itself right so that we are learned then we apply our intuition to the algorithm developing and stuff like that um sometimes I see people from the pury engineering field and they can you know they can use complicated mathematical Solutions in very fluently but they have you know no intuition in what the data should look like so they sometimes do something really nonsensical from my viewpoint um so we we definitely need the balance between these two and another thing is that uh once you get some results from the statistical you know you you follow the statistics to do the data analysis and when you look into the each data using your intuition and learned eyes then you find oh my gosh this is really you know bad signal still remaining and this often happens but remember we are psychologically always very biased and you know we have a very specific biased way you know I have seen many end users saying oh but this algorithm doesn't take care of that and that but they can't show the you know the statistical evidence of it's not working so we are always kind of biased by those outlier bad results and we say oh this algorithm doesn't work but overall you know it works and if if someone says it doesn't work then they have to show some statistical evidence for that so you know those are probably the things I can say for that Linda you get the last question um really great talk I really enjoyed it um I have a question that's pretty specific and I I admit it comes from apparently ignorance I didn't know I held so um I was aware of the um IR related signal in gamma but I actually thought it was it was in the high gamma and the data you showed shows it in low gamma as well I was very interested in in that finding is your opinion that all gamma related signal is IR related in EEG or or what um I I think we can still record the genuine EEG gamma under the well controlled situations um and also careful interpretation and Analysis um for example if we monitor the micr tremo of the eyeballs using the you know high resolution it tracking system then we can significantly you know reduce the probability of misled by the IM movement or sated you know gamma um so these are the real like practical counter measure to exclude the possibility of misinterpretations right otherwise only by using signal processing there there are things we can do like using some very smart algorithm can some times help but um it's it's actually better to use those eye truckers for example to monitor the eye movement these are like the ground truth based um processes so they are all always better than just signal processing Makoto thank you very much for your time and for this talk that was great um we are um we really do appreciate you uh spending time with us we've recorded the the talk and it will be available on Confluence for those of you who don't know what Confluence is um you can email us um and we'll be able to send you the link so thanks very much you're welcome.
# Transcript
![](https://www.youtube.com/watch?v=NHKIPU_T7-0)  

A presentation at NIH HEALthy Brain and Child Development Study (HBCD) summer seminar. Recorded on July 23, 2023.  
  
https://heal.nih.gov/research/infants-and-children/healthy-brain
# Others
## References
+ 