---
title: "What is the best ERP baseline?"
source: "https://www.youtube.com/watch?v=2wS7-XILNso&t=207s"
author:
  - "[[EEGLAB]]"
published: 2023-05-04
created: 2024-12-17
description: "What is the best ERP baseline? Should we even remove the baseline? This subject has been debated for several decades now, and it still is. Let's review the subject.0:00 Introduction2:41 Average ERP"
tags:
  - "clippings"
---
# What is the best ERP baseline
# Notes
# Content 
Video title: Statistics using non-parametric randomization techniques
    
Video URL: https://www.youtube.com/watch?v=x0hR-VsHZj8
    
Video language: English (auto-generated)
    
--------------------------------
Video title: What is the best ERP baseline?
    
Video URL: https://www.youtube.com/watch?v=2wS7-XILNso&t=207s
    
Video language: English (United States)
    
--------------------------------

Hello, my name is Arnaud Delorme.  When researchers study the brain   with electroencephalography, they usually  subtract the baseline activity from each   EEG trial before computing the average.  This video shows why it's a bad idea. First, what do we mean by subtracting  baseline activity? Nope, sorry, that's not it. Look at this person doing an experiment.  Visual stimuli are presented on the screen,   and we record the brain activity of one  EEG channel time-locked to the stimulus.   We stack the trial and average them. Then we  may look at the ERP or average event-related   potential. Now before we average these trials, we  subtract the baseline activity from each trial,   and from each channel independently  if we have multiple channels. This is illustrated here. We want the signal to  be centered on 0. We start with a trial offset   from 0. We compute the mean in the baseline  and remove it to recenter the baseline on 0.   Standard baselines are from -100 milliseconds  to 0, -200 ms to 0, and up to -1 second to 0. Where does this come from? Why correct the  baseline in this way? Subtracting the baseline   is not the only way to correct EEG signal  drifts before the presentation of a stimulus. This method of subtracting the mean for  baseline correction has a lot to do with   how event-related potentials came to be  studied. In the 1960s, researchers at UCLA   decided to use a new machine for studying  EEG, the [[computer of average transients]],  which was originally designed for studying atomic  wave phenomena. This machine is like a modern   oscilloscope. Researchers can trigger acquisition  based on an external signal. It will then store   the time-locked EEG signal and average it over  time. In other words, it will compute an ERP.   The machine did not store the  raw EEG, only the averages. From these early experiments, which were  very successful, came the natural model   that the EEG data is equal to the average ERP  plus some background noise. In other words,   every single trial is a noisy version  of the average. This was in line with   the models of the brain at the time based on  behaviorism, where the brain was considered   a black box that we probe with stimuli.  Behaviorism is gone, but the ERP model stays. The ERP model is true in some extreme  cases, such as direct nerve stimulation.   If we stimulate a peripheral nerve, then we  often have a true ERP. However, in most cases,    the ERP model is a simplified story  of what's going on in the brain,   and we can see that by looking at single  trials. How do we look at single trials? For example, here are four single trials  time-locked to the presentation of a stimulus.   We can encode the potential using color,  with blue indicating negative values and red   indicating positive values. We can stack these  color bars to build what we call an ERPimage.   Here we only had 4 trials, but this image stacks  400 trials. The black trace shows the reaction   time of the subject for each trial. The trials are  sorted based on their presentation order. However,   we can re-sort these trials by their reaction  time, as shown here. Because there are so many   trials, we often smooth them vertically, and when  we do that, some patterns may become visible. We can see some early activity  time-locked to the stimulus,   but only for fast reaction times. We can also  see later activity time locked to the reaction   time. The average ERP is the same in all ERP  images and fails to capture this complexity. The ERP can also be influenced by brain  activity before the stimulus. Here we   have data trials that the subject is  instructed to ignore. We isolated some   trials with high alpha power and others with  very low alpha power. We can see that the ERP   averages are very different even though the  stimulus is the same. For high-power alpha,   the single trials are sorted by the  phase of alpha, and we can see that   the ERP is caused by the alignment of  alpha waves to the stimulus presentation.   So, this shows that the brain's activity  before the stimulus's presentation matters.  Finally, the ERP model contradicts the  predictive coding model of the brain,   which is currently the most widely accepted  model for how the brain interacts with the   environment. In the predictive coding model,  the response of the brain to a stimulus is   seen as the error of its prediction, so the  activity of the brain before the stimulus is   presented matters. In other words, the brain is  constantly trying to predict what is coming next.   By contrast, the standard ERP model assumes  that, before the stimulus is presented,   nothing of relevance to that  stimulus is happening in the brain. So that's a long introduction about the  history of mean baseline subtraction and   where it comes from. Baseline subtraction  is not the only baseline correction   method. For example, it is also possible  to high-pass filter the signal to remove   slow oscillations that offset the EEG  signal before the stimulus presentation.  This is a relatively recent paper advocating  the use of a baseline against filtering. It   is an amusing title, but I think that's  the only thing I enjoy about this paper.   The argument for using a baseline goes as  follows. The authors imagine some simulated   ERP. You will notice the underlying model where  the brain signal is flat before the stimulus   is presented. People might as well throw  their brains in the trash. Let's move on. If you filter the data, the authors  argue that the signal is distorted.   This is for a signal high-pass filtered at 0.5  Hz. Why would a filter affect the data in this   way? Well, digital filters are first applied  in a forward pass and then in a backward pass.   This allows for the cancellation of any  potential phase distortion of the filter. The backward pass is the reason why a  filter will modify the signal in the past,   where the peak here creates artifacts before its   onset. And it is indeed a problem if you  are interested in the onset of the peak. Now I am going to digress again. We saw that  the standard ERP model fails to capture the   brain complexity embedded in single-trial EEG. ERP  researchers will be interested in the latency and   peaks of the ERPs. However, there is usually a  sequence of ERP peaks, so the onset of an ERP,   for example, P300, may depend on the  peak of the preceding one. Similarly,   the amplitude of an ERP may depend on the latency  and amplitude of the following and preceding ones.   I personally believe the onset and amplitude of  ERPs only show you the tip of the iceberg and   might be misleading if you are interested to know  what is happening in the brain. The latency and   amplitude of ERPs may have little physiological  meaning. So, saying that filtering is wrong   because it affects the latency of idealized and  unrealistic brain signals might be misleading. But even if you wanted to measure latencies,  ERP researchers have shown that you can use a   filter that only has a pass forward and no pass  backward. The grey trace is the original signal,   and the green trace is the single-pass forward  filter. This type of filter is also called a   causal filter, as opposed to a non-causal  filter that uses a forward and backward   pass. You see in green that the latency is  not affected, although the amplitude is. So we have shown that pre-stimulus  baseline removal is based on obsolete   hardware and brain models. But  why is it fundamentally wrong?  The main problem is that it's not physiologically  plausible. Here I have a source in the brain that   projects on these electrodes. When a brain source  is active, this is the activity we may record over   time on these electrodes. What is important here  is that the projection of this source onto these   electrodes is proportional to the activity of the  source. For example, if the source activity is 1,   then we record 1.5 on the first electrode,  1 on the second, and 0.5 on the third. Now,   if the source activity is 2, we record three on  the first one, 2 on the second, and one on the   third. This is based on Maxwell equations and the  assumption used by source localization software. Now let's remove the baseline as it is done in ERP  research. You see how the signal is shifted for   each electrode independently. Now, the activity  on the electrodes is no longer proportional to the   source activity. This is an important problem,  and the shorter the baseline, the worse it is. Now, you might think that this is a theoretical  consideration and that it does not matter in   practice, but it does. I could not find a paper  showing that removing the baseline decreases the   quality of source localization, but someone should  write it as it is evident from this diagram.  However, here is a related paper using  Independent Component Analysis or ICA.   ICA is able to find the scalp projection  of brain sources. These authors found that,   when splitting the data in two, ICA failed  to find the same components when the baseline   was removed. This was especially true for short  baselines. This problem arose because removing the   baseline adds some non-linearity to the brain  source's projection, as I showed you before.  Also, let's look at ERP in the frequency domains,  also called ERSP, for event-related spectral   perturbation. Unlike ERPs, when using ERSPs,  the single trial baseline is never removed.   This is a representation of ERSP here, where the  color indicates the amplitude of brainwaves at   different frequencies and times. When it is red,  the amplitude of the brainwave is larger. There   is a baseline here, and it is calculated  independently for each frequency. However,   this baseline is calculated on the average,  not on single trials. We discuss this in   detail in this paper. If we don't do it  for ERSPs, why should we do it for ERPs?  However, now comes the biggest blow  to subtracting the baseline. A major   issue with baseline subtraction  is the effect on significance. In this first experiment, participants  saw photographs, one at a time,   and pressed a button whenever they saw an  animal. This is shown here. We compared   the ERPs for animals and distractors. When  we looked at the ERPs, we picked the range   350 to 450 ms as the one where the difference  between animal and distractor trials had the   highest number of significant channels. Here  about 50% of the channels were significant,   although that varied across subjects,  as shown by the red-shaded area. The rationale for using the percentage  of significant channels is that,   because of volume conduction, an effect  will be visible on most channels,   even if it is localized in one brain area. It  is illustrated here. We have a brain source,   and its potential is visible on most channels. And again, you might think I am exaggerating, so  I want to mention this funny anecdote. This is   a paper from Simon Thorpe, my Ph.D. supervisor,  where he observed some early differences between   images of animals and nonanimals, and the  experiment was the same as the one I just   explained. You see this differential activity  between animals and distractors at 175 ms.   It does look like there is some difference in  the frontal cortex, and it was interpreted as   such in the paper. However, upon adding  more electrodes in occipital regions and   performing source localization, it turned out  to be visual cortex activity in the back of   the head that projected all the way to the frontal  electrodes. So, yes, volume conduction is no joke. Let's now return to the current  study. Our main question was:   if we change the baseline, does that change  the percentage of significant channels? Here you see the general pipeline. For each  subject, we high pass filter the data at 0.5 Hz,   then we extract epochs for two conditions,  in this case, animals and distractors.   We then apply different baselines and  count the number of significant channels.   To be able to create confidence  intervals, we randomly select 50   trials of each condition repetitively.  This is indicated here by "bootstrap." We do that for each subject, and this  is plotted here. Each dot is a subject,   and the ordinate represents  the difference in significance   compared to the raw signal after  high-pass filtering at 0.5 Hz. So the two points here at about -17 indicate two  subjects for which the percentage of significant   channels decreases by 17 when you use a 100  ms baseline. The percentage of significant   channels is 57% on average, so removing 17 means  only 40% of the channels remain significant. So,   we lose about 1/3 of the significant  channels when applying a 100 ms baseline. On top of these dots, we also show  the mean and the standard error. So what happens with other baseline  durations? The effect is not as dramatic,   but all baselines perform worse  than not using a baseline. We have also tried this method  in two other experiments. This is a popular face categorization  dataset available online. In this one,   we compared familiar faces vs.  scrambled faces, as shown here.   The maximum number of significant channels was in  the 250 to 350 ms time range. We found that, as   for the first experiment, all baseline subtraction  performed worse than not applying a baseline. Finally, we use EEG data from an auditory  [[oddball task]], so imagine beep, beep, boop,   beep, and the task of the subject is to press a  button on the boops only. Here is how it sounds.   The beep sounds are frequent and called  standards, the boops are rare and called   oddballs. In this kind of oddball task, we  usually observe a difference between the ERPs   of oddball and standard. Here we looked at all the  channels and saw that in between 400 and 500 ms,   about 50% of the channels were significant.  We saw that most baseline corrections larger   than 500 ms led to a significant decrease  in the number of significant channels. The take-home message is that, in terms  of reducing the noise in the data,   not subtracting the baseline is probably best. Note that, for these analyses, the data was high  pass filtered at 0.5 Hz before applying baseline   correction. Because of the signal distortion  of digital filters we mentioned earlier,   a 0.5 Hz cutoff is not what ERP experts recommend.  Instead, the ERPLAB software package suggest a    0.01 Hz high-pass filtering of the raw data and  then a 200 ms baseline after extracting epochs. So let's compare this approach to a  standard high-pass linear filter at 0.5 Hz. We found that you lose about 20 in the  percentage of significant channels for the   animal categorization dataset and more than 40%  for the oddball dataset. For the face dataset,   no significant difference is observed. However,   this dataset had a 0.1 Hz high-pass  filter applied during data acquisition. Now let's emphasize the results on the  oddball dataset. Here you go from about   60% of channels significant to about 30%,  so it means half of the channels that were   significant stop being so when you use the  standard ERP approach. This is not subtle. Is this the end of the story? Do we simply  stop subtracting the baseline activity and   high-pass the data instead? Research is a  process, and this will hopefully spur some   discussion and more papers in the community.  If you feel you need to use a baseline,   use one longer than 500 ms, as short baselines  are the most detrimental to data quality. That's it for this video.  Thank you for your attention,   and I hope to see you in one of my future videos.

# Transcript
![](https://www.youtube.com/watch?v=2wS7-XILNso)  

What is the best ERP baseline? Should we even remove the baseline? This subject has been debated for several decades now, and it still is. Let's review the subject.  
  
0:00 Introduction  
2:41 Average ERP vs. single trials  
4:25 Theoretical issues with the ERP model  
5:18 Can filtering replace baseline subtraction?  
7:54 Source localization and baseline subtraction  
9:15 ICA and baseline subtraction  
9:46 Baseline subtraction time vs. frequency domain  
10:28 Baseline subtraction and statistics  
15:30 Baseline vs. filtering  
  
References:  
\------------------  
Delorme A. EEG is better left alone. Sci Rep. 2023 Feb 9;13(1):2372. doi: 10.1038/s41598-023-27528-0. PMID: 36759667; PMCID: PMC9911389.  
https://www.nature.com/articles/s41598-023-27528-0  
  
Tanner, D., Norton, J. J., Morgan-Short, K., & Luck, S. J. (2016). On high-pass filter artifacts (they're real) and baseline correction (it's a good idea) in ERP/ERMF analysis. Journal of neuroscience methods, 266, 166–170. https://doi.org/10.1016/j.jneumeth.2016.01.002  
  
Grandchamp, R., & Delorme, A. (2011). Single-trial normalization for event-related spectral decomposition reduces sensitivity to noisy trials. Frontiers in psychology, 2, 236. https://doi.org/10.3389/fpsyg.2011.00236  
  
Groppe, D. M., Makeig, S., & Kutas, M. (2009). Identifying reliable independent components via split-half comparisons. NeuroImage, 45(4), 1199–1211. https://doi.org/10.1016/j.neuroimage.2008.12.038
# Others
