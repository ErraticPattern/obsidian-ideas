---
title: Statistics using non-parametric randomization techniques
source: https://www.youtube.com/watch?v=x0hR-VsHZj8&t=68s
author: fieldtriptoolbox
published: 2015-04-22
created: 2024-12-16
description: Lecture by Eric Maris during the "Advanced analysis and source modeling of EEG and MEG data" Toolkit of Cognitive Neuroscience at the Donders Institute.
tags:
  - clippings
---
#
# Notes
Very interesting interpretation of [[Data analysis]] where it is seen as a part of statistics.
# Content 
Video title: Statistics using non-parametric randomization techniques
    
Video URL: https://www.youtube.com/watch?v=x0hR-VsHZj8&t=68s
    
Video language: English (auto-generated)
    
--------------------------------

okay welcome so I I'd like to take you through some aspects of statistical testing of [[electrophysiological data]] so this is a brief outline so I will start with an attempt to try the question why we need statistics in neuroscience and that will involve aspects such as a difference between inferential statistics and data analysis so I will mention the details of a fairly unknown statistical test it's called the [[intraocular traumatic test]] and I will explain why it is useful to understand [[inferential statistics]] or briefly say something about the general idea of the heart inferential statistics and the general idea is in a nutshell that is principled decision making under uncertainty and I will then discuss two different approaches to inferential statistics and the first one most of you will know so-called [[Neyman-Pearson approach]] it's the approach that most of you had in introductory statistics courses at The Bachelor level and then I will go into the permutation based approach which will be novel to some of you but at the same time has some remarkable similarities to the name on Pearson approach but also a few crucial differences and these crucial differences turn out to be very useful for Neuroscience applications and finally I will wrap up  okay first question why do we need statistics in neuroscience so there are actually two ways in which statistics helps us first it helps us in making decisions under uncertainty and the corresponding branch of Statistics that helps us with this question is called inferential statistics and you all know it via its test statistic like these statistics F statistics chi-square statistics they're all part of this branch of inferential statistics they're about decision making now there is another Branch about which I will not talk but which most of you also know about it's so-called data analysis and that part of Statistics is used to review patterns in the data which you cannot identify by eyeballing so very often we have high dimensional data and just by looking at these data or by summary statistics of these data we cannot see revealing patterns in them and therefore statisticians have come up with tools like factor analysis principle component analysis Fourier analysis that all help us to identify revealing patterns in the data importantly that branch of Statistics doesn't involve any p-values at all but nevertheless it's very useful of course there are also mixtures of the two data analysis with p-values attached to them so today I will only focus on inferential statistics so why do we need statistics in  Neuroscience now in 1950 there was a famous statistician barracksome and in a challenging paper he introduced the idea of the intraocular traumatic test and this test works as follows when you look at the data the conclusion hits you right between the eyes and this expresses very well why we need statistics in Neuroscience we need it because this test very often fails because there is uncertainty in the data pattern now there are cases not so uh not so frequent where actually this interocular traumatic test would work and let me give you an example say you have a sample of 20 participants and all of these 20 participants show exactly the same difference between two conditions A and B and the same difference means the same difference across space the same channels or the same voxels in Source space the same frequency band say all between 12 and 14 Hertz and the same time interval relative to some stimulus say all between 300 and 400 milliseconds in that case there is almost no uncertainty in your data pattern and that's when you say the conclusion hits you right between the eyes now assets the intraocular dramatic tests very often fails because either the Observer is uncertain about the conclusion that can be drawn so you as a as a scientist looking at your data for the first time you don't know how to make sense out of it whether there is a real pattern or a real difference between the two conditions or not or that's also very likely different observers looking at the same data pattern they disagree with respect to the conclusion and that very often happens between you and the reviewers or between you and imaginary reviewers because you look at your data and you think I think this is in the data but will some other guy in particular the reviewer will he also conclude the same thing from that pattern of data and in in and then you want to settle the issue either interaction with the reviewer or beforehand so you do it with your imaginary reviewer and statistics is useful for that  um so why does the intraocular automatic test so often fail actually I briefly mentioned already it has to do with uncertainty and there are actually basically two types of uncertainty first of all there is the uncertainty that is captured under the label signal to noise ratio and every one of you has already collected neurobiological data be it fmri or electrophysiological measures you will find out that the SNR is typically very small especially at the single trial level it's definitely not the case that neither fmri or electrophysiology is a window on the brain in the sense that a layperson would understand this just looking at the data and then observing revealing patterns uh in the ongoing data that's not how it goes SNR is just not sufficient for that seconds in neuroscience and that's very different from Behavioral Science we work with high dimensional data structures and these high dimensional data structures they are compared between conditions and because they're so high dimensional you cannot do this on the basis of visual inspection alone so compare this with the pure behavioral experiment typically every trial gives you two numbers either you press the left button or the right button and you have a reaction time so you have two numbers for every trial now if you take a subject and put them in the Meg scanner downstairs we have 275 Channel system and we typically record that they say sampling rate of a thousand Hertz so every second gives you 275 000 numbers and compare it with two numbers and then also we have a lot of scientists especially here in the domus have an interest in frequency and they create an additional Dimension there they create a frequency Dimension you get a three-dimensional uh structure you get channels by frequencies by time so that becomes huge so in an EEG and Meg if you compare two experimental conditions with spatial temporal data you have one comparison for every sensor time pair and if you have a frequency axis squeezed in between you have triplets and you have tenths or sometimes hundred thousands of these triplets the same problem holds for fmri of course but there only the spatial Dimension creates the high dimensionality but there also you have much more voxels than you have channels  okay next question do we have something like statistics for neuroscience and the brief answer is no I think the general principles of statistical decision making the ones that we use for analyzing behavioral data we also use the same principles and Analysis of neurobiological data however Neuroscience has its own statistical problems and because it has these problems it requires specialized methods and most of these problems they are due to the fact that we deal with very high dimensional data and that brings us to the multiple comparison problem so we typically in every experimental comparison between two uh conditions we have tens to hundreds of thousands of comparisons and each of them you can make you can make them either using a t-test or an f-test depending on the number of levels of your independent variable the total number of test statistics that you would evaluate at the level of these sensor time pairs or a sensor frequency time triplets each of them produces one p-value if you do it using the standards standard statistical approach and I probably don't have to explain that this uh if you do if you do this naively this results in a drastic increase in The False Alarm rate  what is good very good to keep in mind to keep in mind as a way to not overvalue inferential statistics you should keep in mind is that it is principled decision making at the end it's about making decisions and you do this in a principled way so you choose a particular principle that allows you to deal with inherent uncertainty in your data so the the essential points are that uh this inferential statistics is based on rational principles decision making based on rational principles and very importantly there is not only a single rational principle there are more rational principles there are not very many because it's it's not so easy to come up with a principled decision making so you you have to think about this carefully and for that we have our colleagues the mathematical statisticians they they look for principled ways of dealing with the uncertainty in our data but there is more than one and I will give you a list in the following crucially you can arrive at different conclusions depending on the principle on which your decision is based and in that sense you cannot say that statistics is about the truth or that statistic reveals you the truth statistics is not about revealing the truth statistics is about correctly applying a particular principled way of dealing with the uncertainty in your data now let you give give you a list of the four most common ways of dealing with uncertainty in your data the name and Pearson approach which I will go into in some detail and maybe you don't know it under this name but these are the concepts that you are familiar with it's like the null hypothesis the alternative hypothesis type 1 type 2 error and so on there is false Discovery rate control it's very popular in Neuroscience I will not say anything about it I will I will treat a permutation based approach which may be new to some of you and there is the nowadays extremely popular Bayesian approach um all of them are in principle sound but they might lead to different conclusions in particular the Bayesian approach some of you who are familiar with the ingredients of this approach and you know that there is a role of the prior distribution for the conclusion that you arrive at by giving the prior distribution a very strong weight in your inference in principle you can arrive at any conclusion which is not wise of course because the prior should only reflect genuine prior knowledge but there is an aspect in the Bayesian approach that um yeah for some people uh is being conceived as subjective okay so how will I go about this so I for each of the two decision making principles that I will deal with name on Pearson and permutation I will illustrate the statistical testing procedure actually the operational procedure and I will do this by means of a very simple example so the difference between two means and no no neurobiological data at all just a simple say simple reaction time experiment and two experimental conditions I will explain the rationale behind the procedure then I will go to neurobiological data and show how the multiple comparison problem is solved under that principle and I will then evaluate the solution so this is how it goes in the name on Pearson approach you always start from a so-called null hypothesis and this null hypothesis always pertains to unknown population parameters and typically the the null hypothesis is about expected values or population means you also have null hypothesis about variances correlation coefficients but the story is the same so here we focus on uh expected values and you have an interest in whether the expected values in two populations typically two experimental conditions A and B whether they are equal or not so you formulate your null hypothesis and then you take some test statistic and I'm not going to say a lot about this at the moment how you have to choose your test statistic but there are ways to select the test statistic and the most familiar one is the two sample T statistic that's the one that you use if you want to evaluate the null hypothesis of equal means in two conditions two populations and the last point and that's the one where you have to think a little bit about it because also a bit longer sentence what you should do is you should find the critical value for your chosen test statistic such that under the null hypothesis the one that you formulate here the probability of exceeding this critical value is controlled that that is that is the principle According to which the name on Pearson approach uh makes database decisions so how it deals with the uncertainty in the data it starts from a null hypothesis it looks for a certain test statistic for that test statistic it uh it knows the probability distribution under the null hypothesis and in the decision making procedure one can make use of that known probability distribution of that test statistic under the null hypothesis and from that probability distribution which is called the sampling distribution the sampling distribution under the null hypothesis from that probability distribution we can arrive at the critical value and that critical value is being defined by some critical p-value typically 0.05 under that sampling distribution and if you have a t-statistic that exceeds this critical value then you say that this null hypothesis is too unlikely given the observed data and rejected okay so that's the procedure and a little bit of the rationale behind it there are some constraints to that approach and these constraints will turn out to be very important when you go to neurobiological data first of all the probability distribution of the test statistic under the null hypothesis has to be known yeah so you must at some point when you pick a test statistic you must be able to prove what is the probability distribution of that test statistic under that null hypothesis and that turns out in general a very very difficult problem which is not something for neuroscientists to solve it's something for mathematical statisticians to solve a mathematical statisticians they have built the so-called normal theory of the normal the normal distribution based Theory they have shown that for a whole family of test statistics of which these statistic is the most important one it is possible to derive the probability distribution under the null hypothesis but you must make some auxiliary assumptions first of all you must assume normality of the data you must assume that the variance of these data in the experimental conditions being compared that this variance is equal and it assumes Independence of the observations later when we discuss the permutation-based approach we will drop those two assumptions but we will keep Independence so all the statistical procedures I know up all of them I say 95 of what I know of assumes and Depends and we will also do it here now a second constraint which is definitely as important and also very crucial for Neuroscience is that the test statistic that you choose should not be just any test statistic you want to test statistic that is sensitive you want a test statistic that has a very high probability of rejecting the null hypothesis when it is in fact false because that's what makes us happier we want to reject null hypotheses and for that we want to have a test statistic that is really sensitive to the pattern in the data that we consider to be informative and that we hope to elicit using our clever experimental manipulations so expressed in the language of Statistics we say that under the alternative hypothesis the probability of exceeding the critical value must be large we also say that the test must be powerful or must be sensitive it must have a small type 2 error it's a high sensitivity yeah for neuroscientific data that turns out to be an important challenge I will return to that so what I have set until now I'm going to try to present this graphically and I hope that this will also make the concepts behind the name on Pearson approach a bit more Vivid and it requires some notation it's not so much but please I have only two or three slides of this type try to follow me in understanding the symbols there aren't so many so I'm going to start at the lower right that's where the variables are being defined and I have two types of variables I have so-called random variables and I have realizations so whenever we do statistics we consider our data as being generated by some stochastic process that's being depicted by this capital D capital D it denotes the fact that if we would replicate our experiment under under additional sorry under identical conditions we might observe a different value and the exact value that you record is a value small D and this small D corresponds to a random variable capital D that's the difference between big and small D so there is some Randomness when we generate a particular number if You observe a particular reaction time in a particular trial and we we do not consider this to be a deterministic quantity we assume that reaction time could have been different if we would have repeated the experiment under identical conditions so that's the difference between Big D and small D now you see there is a big Vector here and this big Vector it just denotes all the data in my experiment which involves two experimental conditions capital A and capital B condition a and condition B and every each of the two conditions has small n observations so I have small n observations in condition a and small and in condition B yeah and to make this concrete just consider a reaction time experiment involving a single subject who has small n trials in one condition and small n trials in another condition so this is now we're going to do a t-test on the data of one subject so later I will also show you how to use the same principle if you want to do the typical analysis over subjects and then you calculate average reaction times within every subject but for for explaining the principle behind name and Pearson Theory doesn't matter whether you look at the data of one subject and then the replications are the trials or whether you look at the data of multiple subjects and then the subjects or the replications so we look now at the data of one subject now there's one subject it has generated numbers two times n numbers and numbers in condition A and N numbers in condition B so this is just the vector of numbers that we have in the model of on in the Matlab workspace and we can now run the multiplop function t-test and that will give us a p-value and the rationality behind this p-value calculation it is being depicted here what is be what is being shown here in this rectangle that's the null hypothesis that we want to test and the null hypothesis is the following it says that this vector-valued random variable d with n numbers from condition A and N from condition B that this Vector values random number is drawn or or involves or is characterized by expected values mu a and mu B which are equal so if so I have here n numbers or n random variables condition a and you're in condition B each of them has an expected value and in condition a they all have the same expected value mu a and here they have the same expected value mu B and what we want to test now is whether these expected values are equal and then we have some auxiliary assumptions like equal variance in the two conditions and we have normality so that's what we want to test now what is the logic behind the test that we perform the logic is that we calculate a test statistic and the test statistic is here being denoted by S now you see there are some of these symbols that involve gray and I'm I'm going to explain them in a minute I'm going to start with the observed data the the non-gray small D those are the numbers that we have actually collected I calculate the test statistic which which is going to be my two sample t-test and then I'm going to evaluate that test statistic s under the reference distribution which is going to be my T distribution with 2 times n minus 2 degrees of freedom and I and I evaluate whether that observes test statistic is somewhere in the tail of that probability distribution the question now is where does that probability distribution come from and that is being explained by these gray numbers that you see here these gray numbers they're D1 had the two hats they're all non-observed they only exist in theory but these gray numbers D these are the numbers that would have been observed if we would have drawn data from the null hypothesis in practice we do not know whether the null hypothesis is true for the variable d which govern our data but the game that we play is we think or assume temporarily that the null hypothesis is true then if the null hypothesis is true we can collect imaginary data which are these Gray symbols for each of these gray symbols we have a test statistic and if we would then have an infinite number of these test statistics make a histogram of them then we will get our reference distribution our T distribution with 2N minus two degrees of freedom so that's the idea so the gray numbers are the ones that generate your reference distribution and the black number evaluated at its test statistic it's the value that you're going to use to find evidence against this null hypothesis if that test statistic is far in the tail of the distribution that you get under the null hypothesis you find it unlikely the null hypothesis to hold okay so that's the idea behind it and now let's see what we have to do if we want to apply this not to scale our observations reaction times but we want to apply this to a data set which has hundreds of thousands of samples like an Meg data set now how you go about that what you do now is you formulate your null hypothesis in terms of unknown population parameters at all elements of the multivariate random variable D so this random variable D now is not just a scalar one number per trial but it is tens or hundreds of thousands of numbers per trial but still it is some random variable that has some expected value but that's a huge array yeah an electrophase it pertains to channel frequency time triplets and for fmried pertains to voxels on the Second Step it's also the same as in the univariate case but with different data structures you calculate some test statistic that depends on the data at all the elements jointly and what has been very popular in fmri is to use the so-called maximum t-satistic so you have in fmri you have uh then few ten thousands of voxels and for each of these voxels you calculate a voxel-specific t statistic and then you aggregate over the voxels by taking the maximum T statistic there's also other types of Statistics that are based on thresholding and calculating the size of the largest cluster so the different types of different types of ways to aggregate the voxel-specific T statistics the third step once you have this aggregate statistic you find the critical value for this test statistic such that under the null hypothesis the probability of exceeding critical values control that's just the same as in the usual univariate case you know the main thing is that you have this very unfamiliar death statistic in between it's not a t-statistic but it's a maximum of a very large number of these statistics or some other way of combining these these statistics now let's evaluate this um first with respect to the constraints that your auxiliary constraints that you have to assume that turns out to be quite challenging it turns out that if you want to calculate the sampling distribution of that maximum t-statistic or some other aggregate statistic you have to make auxiliary assumptions which are more strict than what you have in the univariate case and some of you may have heard about the [[Gaussian Random Field|gaussian random field]] and that is the main auxiliary assumption so if you assume that a gaussian random field holds for your data then you are safe to apply or then you can apply reference distribution for for instance the maximum T statistic a second point just as important or maybe even more important the test statistic for which you know the distribution under gaussian random field that distribution may have a very low power question is what is this power how can we how can we calculate it this is a very difficult question but for the rest of the talk it's very important to keep in mind that it's not only important to have this aggregate statistic such that you know it's a reference distribution under the null hypothesis you also want to have a test statistic that is sensitive for the type of pattern that you are likely to encounter in your data or that you hope to encounter in your data  okay now I have to make a very brief sidestep now before going to permutation based inference now that's about the difference between at the one hand between unit of observation designs and within unit of observation designs because in practice we often analyze data of individual subjects but we and then but when we report in the journal article we report about the results obtained of a large number of subjects and for that we have to make a distinction between these two types of design um whenever I say unit of observation for your own understanding you may also replace this by subject or trial either a trial is a unit of observation or a subject is one now what is the The crucial distinction between those two types of designs um okay that comes in the second Point first point to be made is that in practice we work either with collections of Trials which are which what that's what you have in a single subject study or you work with collections of subjects and that's what you have in a multi-subject study so trials and subjects are jointly denoted as units of observation now there are two types of experimental designs so-called between unit of observation design and within unit of observation design and in a between unit of observation design every unit of observation is observed in only a single experimental condition whereas in a within Union of observation design every unit of observation is observed in all experimental conditions so a single subject study with multiple trials it's always between unit of observations because a given trial is only observed in one experimental condition this does not have to be the case when the unit of observation is a subject typically in the experiments that we run here every subject is being observed in multiple experimental conditions so we do we have task where attention is to the left of the visual field the right of the visual field we have task where we give a q or no Q we have high working memory load low working memory node with motor preparation or without motor preparation and so on and and typically the experiment that we run is such that in one experimental session trials are being collected of the multiple experimental conditions at the end of the experiment you calculate the average across the trials within every condition and so for every sub for every subject you have at least two numbers two numbers one for condition a one for condition B you also have between subjects designs and that's what you have if you compare different groups like if you compare young and old if you uh if you if you compare trained subjects with novices and so on in that case every subject only provides data of one condition and that is from a statistical point of view the same as when you would be analyzing a single subject study because in a single subject study you have trials which are also being collected in different experimental conditions and that's why these two sample t-test is both used to analyze single subject data it's also used to analyze multiple subject data where each subject comes from a different group but that's statistically just the same framework so having that in mind I should tell you something now about the within unit of observation design and that's that's the type of design that you deal with when you have multiple subjects and in practice you use the paired samples T statistic for it or it's also called a dependent samples T statistic and actually the story is just the same as with the between unit of observation design the only thing that is different is that the the the variables that you put in they have a little bit of a different structure but the way you think about it is just the same there are actually no new Concepts here except for the definition of the input and now what you see here is that as compared to the between unit of observation design now the data come in pairs now you see every row here is one subject and I have small n subjects and every subject has been observed in condition a and in condition B the data common pairs that's the crucial difference and because the data come in pairs you analyze them in a way that respects the fact that the data come in pairs that's also reflected in the in the auxiliary assumption that you make now you assume the so-called bivariate normality as your auxiliary assumption and under that bivariator normality your dependent sample T statistic has a t distribution with n minus 1 degrees of freedom okay are now going to introduce the  permutation based approach starting from the name on Pearson approach I'm going to come to focus on the differences there are just a few crucial differences but for applications it makes a lot of difference so how does it work I take the three steps that I also used for the name and Pearson approach I start with formulating a null hypothesis and now the null hypothesis crucially does not pertain to the expected values but the null hypothesis pertains to probability distributions probability distributions of the observations the null hypothesis pertains to the to this capital D variables this capital D is the random variable it has a probability distribution and what we're now saying in our Nola path is claiming in our null hypothesis does not pertain only to the expected value of D no it ex it pertains to the probability distribution of d as a whole so what we're saying now is the probability distribution of the observations in a different experimental conditions they're identical so that now becomes our null hypothesis with two experimental conditions each governed by one probability distribution and the null hypothesis you want to test is that they're identical these probability distributions now second step we take some test statistics and and now there is no constraint about whatever test statistic you choose it's it turns out that you can pick anything that will I will come to that later and third step and that's again very similar to name on Pearson approach you find the critical value for that chosen test statistic such that under the null hypothesis in one here identical probability distributions that the probability of exceeding this critical value is controlled just like name and Pearson now this critical value is defined by some critical p-value under now it's not a sampling distribution it's under the permutation distribution now what are the differences with the name and Pearson approach I already briefly mentioned first the nilopotus is now about the probability distribution of the observations and not about the unknown population parameters second that's very important the p-value under the permutation distribution you can calculate this for every test statistic without any auxiliary assumption so there's no normality or whatsoever and there is also the opportunity to uh to go for a particular test statistic that is uh chosen specifically for the type of effect that you hope to encounter and that's actually mentioned in point three if you want your test to be statistic if you want your test to be sensitive for the particular aspect of your data and I assume you are well informed about the neurobiological mechanism that you are hunting for if you know a particular pattern in your data that's what I want to detect then in prior to collecting your data you can come up with some test statistic that on the basis of a priori arguments is likely to be very sensitive for that pattern if you look for a particular type of coherence between rights temporal and left frontal areas then you more or less know how you have to devise a particular test statistic you can make your test statistics such that it's optimally sensitive for your favorite phenomenon and the permutation distribution will give you uh a valid p-value to test to test this null hypothesis equal probability distributions with optimized sensitivity for your type of effect okay this is the rationale the formal rationale behind permutation test and it looks very much the schematic looks very similar to the name on Pearson approach there's only a few differences there is a difference here this is where the null hypothesis is and there is a difference here this is the way that you arrive at your reference distribution so I'm going to start here so here that's the null hypothesis that you test and the null the null hypothesis is pertains to equality of the probability distributions in the two conditions that you are comparing and I have the I've written down a conditional distribution here I'm going to say a little bit about it this so this small part in my slide it's a little bit of mathematical Beauty in the permutation based based approach I'm going to try to give you just gist of it but if you want to know more about it you should go a little bit in the into the literature so what is being depicted here it's a conditional probability distribution and what you see behind this uh this vertical line that is behind the vertical line it is the event on which you condition so it's a conditional distribution of my data given the unordered observations so this D is one vector in between unit of observation designs two times n numbers and what is behind the vertical bar it's the same numbers but then unordered the same numbers but ignoring their order which involves that the observations in condition a could just have well been at the locations where the observations for condition B had to be now crucially what you can prove in a very short but very elegant proof is that if the random variables in condition a and those in condition B if they have the same probability distribution then this particular conditional distribution the ordered data given the unordered data that's a uniform probability distribution so under that hypothesis the null hypothesis we want to test it holds that this conditional distribution the order data given the unordered data is uniform it means that each of the ordered data given the unordered data is equally likely and drawing from an uniform distribution is very very easy it's trivial you can write this in one line of code every ordering of the two n data points is equally likely so we can just use pseudo-random number generation to create different random orderings with all with equal probability each of these random reorderings with equal probability we can feed them into our favorite test statistic construct a histogram that's a reference distribution under this null hypothesis we have our observed data we eval we look how far it is and the tail extract the p-value and make a decision perfectly valid to evaluate this null hypothesis very quickly for a within unit of observation design it just works the same except that now the permutation is not achieved by swapping the condition a and the condition B unit of observations without respecting any structure here in the within unit of observation design we have this structure that every subject is being observed in two conditions and we have to respect that structure in our permutation procedure and this involves that we only permute the elements within every subject so for condition for subject one we can permute those two for condition for subject two we can promote those two but it's not allowed to permute those two you cannot you cannot replace the a data point of subject one by a data point of subject two but for the rest the logic is just the same  okay now how it works this in practice Yeah uh and what I'm going to show you now is is a particular test statistic it's a I call it a homemade test statistics which would I would all advise you to do to make homemade test statistics in a way to improve your sensitivity and and the homemade test statistic that I uh have built here it's also built in into field trip it's based on clustering it's based on clustering adjacent time points yeah and that makes a lot of sense from a neurobiological perspective if you have so in a in a Time locked trial if you have observed an effect say at 302 milliseconds post stimulus onset and there is an effect there most likely it will also be there at 303 304 305. so you should not consider these samples separately you should combine the evidence against the null hypothesis across those adjacent samples the same holes for space If You observe an effect over P3 most likely pz will also give some evidence again that null hypothesis so it really makes sense to Cluster the evidence across time across space and across frequency and here is a simple example where we only cluster across time and it's the also the data that you are going to analyze you can actually reproduce exactly this this figure here this is this is the this is the we call it an Inca Vapor study it's the one of the first magnetic N400 studies that we have conducted here I have taken one channel here it's a it's a left temporal Channel um uh left temporal Channel there are two experimental conditions this one is semantically congruent and the other one is semantic incongruent so you all know the the N400 phenomenon I think yeah should I explain something about no I cannot do this okay it's two experimental conditions and what what we observe is that in the semantically incongruent condition you see a nice evoked response which starts around 300 milliseconds which which has died around 800 milliseconds so there is so the experimental effect that we're talking about is the difference between the solid line and the dotted line now how can we go about to quantify the evidence against the null hypothesis for this difference so what we do in the first step we calculate these statistics we calculate these statistics for each of the time points and I think we have the sampling rate of 600 Hertz so we have we have 1200 points here 12 1200 T statistics and the naive way to go about here is just to calculate the T statistic for each of the each of the time points then you see hey there is oh there are significant here for uh from 400 until 600 or from 4 until 700 but that's significance at for each of the individual samples and you may imagine that if that if that Peak is a little bit uh less High then you may start doubting hey is this uh as a whole is that does that exceed the critical value enough to reject the null hypothesis yes or no and this this shows you the multiple comparison problem that you get into so now how how does the cluster based permutation deal with this the cluster-based permutation makes use of the T statistic as a thresholding device it does not use the T statistic to calculate the p-value whether it use the D statistic as a kind of a standardized effect measure and it it chooses a particular Criterion value a threshold value a threshold value uh to identify time points that have evidence against the null hypothesis and so you see we threshold it here at 1.96 we could have taken any any number you could take two or two and a half it doesn't really matter for the statistical properties of the eventual test of the of the permutation test it's just you need some thresholds to identify time Windows where there is evidence against the null hypothesis and all these timepend those time winners I have depicted them here these are all the time Windows where they are whether this t-statistic exceeds in either direction the critical value now I have these time windows and I'm now going to combine the evidence across all these time windows in one number and the way I'm going to do this is by making use of the assumption that evidence against the null hypothesis occurs on adjacent time points so what I will do is I will look for contiguous time windows and I will take the time window that is the longest it turns about to be this one there's also one here one here one here one here but they're very small but this guy is very big and I calculate the area under the T curve here and I take the maximum area under the T curve and that's going to be my final test statistic so with this number the area under the T curve under the threshold it's t curve that is going to be my favorite test statistic I like it because it takes into account that any genuine evidence against the null hypothesis should be contiguous over time and the same thing holds for space and frequencies so now how do we arrive at the uh at a reference distribution to evaluate whether that observed area under the Curve is large enough to be sufficient evidence against our null hypothesis and that's something that we achieve by randomly permuting the congruent and the incongruence trials it goes as follows if you randomly now permute congruent and incongruent trials and if you would then look at the difference between the dotted and the dashed line they would only be random differences because if you randomly permute congruent and incongruent you're just destroying the effect so after random permutation the solid line which does now not consist of genuine congruent trials it's just half congruent half incongruent and the same holds for the dotted line yeah both of them they would be about at the same more or less the same trajectory and because they have more or less the same trajectory the corresponding T statistics they would yeah they would be that would be centered at zero and that would ball approximately five percent of the samples would exceed uh plus 1.96 and or minus 1.96 so after random permutation you would see this t-statistic time course to wiggle a bit like this now if you have this Wiggly t-statistic time course without any pronounced Peaks and troughs we use that Wiggly T statistic time course to calculate exactly the same test statistic that we also calculated for our observed data so we're also going to threshold at 1.96 and minus 1.96 identify contiguous time intervals calculate the maximum area under the curve and that maximum area under the curve that will then go into our reference distribution for our test statistic we do this a very large number of times get a histogram calculator p-value using the observed statistic here that's how it works and here summed up the procedure  so it works in it involves six steps first step is to calculate the T statistic for each of the samples in the multi-dimensional data structure and we have considered this case only temporal data time samples but you can do it just as well using spatial temporal data or spatial spectral temporal data you threshold this sample specific statistics you construct connected clusters of these samples across time space time frequency space clusters that exceed the threshold that all have the same sign if you do this separately for positive and for a negative uh clusters then you calculate a cluster level statistic by taking the sum of the sample specific T statistics you take the maximum of these cluster level statistics and you evaluate this maximum under its permutation distribution that's all this is an example of how you do this with spatial temporal data it's just the same story except that now you also have a spatial Dimension and the Clusters that you get are not only temporal cluster but the result of the low SNR of most biological signals and also the high dimensionality of its data structures I believe that Neuroscience cannot do without statistics despite the fact that it will not allow you to reveal the truth you have to do something with the uncertainty in your data third every decision-making principle that you use in Neuroscience must be able to solve the multiple comparison problem fourth I believe that permutation tests are ideally suited for Neuroscience for two reasons they allow you to solve the multiple comparison problem without making auxiliary assumptions and second they can increase sensitivity by incorporating biophysically plausible constraints in the test statistics and have done some simulation studies and if you go to spatial temporal data the advantage of permutation-based methods especially in combination with clustering it's spectacular you can just try out for yourself by running a small simulation study this is a work that I have done with  colleagues where we describe the methods and some references of applications actually quite old  turned out to be an extremely popular method nowadays I think it's cited almost once every day I'm very happy about this about about about eight years ago we started with this and then we actually I advertised it here in exactly this course and we had two citations in the first year and so you see by by teaching in a in a field trip courses really helps in getting cited so that's all so enough jokes  questions which one yeah yes yes yeah your question is a very good question I I always get this question and I also uh in the psychophysiology paper that I refer to I also spend a few paragraphs on that issue oh yeah thank you thank you very much thank you very much thank you foreign asked the following uh different from this example here where there is a very clear a deviation uh from what you expect under the null hypothesis at one particular time interval it might also be and I have also observed that in the same experimental contrast it is not just one time interval that turns out to be uh to exhibit such a strong difference against but multiple exhibit a strong difference and you also have this over space and some experimental contrast you have uh a left frontal effect and a right sensory motor uh effect s doesn't have a good answer to that what statistics can do very well is making decisions about null hypotheses and now remember the null hypothesis is that we are testing here that null hypothesis is a very Global null hypothesis and in this case if we look here at this one time series at one channel cortex but now assume you have uh you're running experiments with professional subjects you don't have to pay them and you run not a session of one hour but you have them return every day and you have collected hundreds of thousands of Trials from them from a series of 30 consecutive days and then you do this where all of the people in this lab had to be a subject every every two days and that for a couple of months and then very simple sensory motor tasks also the Bold response and they it's something like instead of moving your uh index Fame where you were moving your pinky at some point you could also find significant differences in the frontal cortex so this just occur but the uncertainty in that spatial temporal spectral localization of the effect that uncertainty cannot be Quantified by a p-value or by a confidence interval so and it's a very long story because I also try to answer the the ideas behind it and I was a bit prepared because I just got the question uh every year it's a very legitimate question and we just the bottom line is don't use statistics for everything use statistics for rejecting a null hypothesis but then for characterizing the spatial temporal spectral structure of your effect use other methods like source source reconstruction for instance and yeah clever experimental manipulations and not although um but those interaction effects that are most common in Neuroscience can be tested and that is because most of these interaction effects they pertain to within unit of observation designs so now you have typically have you have a factorial structure in your experiment you not only have two experimental conditions but you have two by two experimental conditions actually have four experimental conditions and you want to know whether the effect of independent variable a is different according to the levels of independent variable B and the way you actually do this in your calculation is that you calculate difference scores mental variable B at each of the levels of variable a so you have two different scores and if you now want to compare the average of these two difference scores is just again a paired sample t-test but it's now a paired samples t-test using difference scores and not using within single condition observations and the whole logic applies when doesn't it apply it doesn't work if you want to test interactions in between unit of observation designs so if you have an experiment say you have a male and female and you have young and old respect so actually it's a bit more complicated than what I tell you now but this is this is the gist of the idea yeah on the difference sure then you have an independent samples t-test again at the level of a different scores so you calculate the different score for every subject then you say you're still controlling your type 1 error rate but this is with three test statistics that you have come up with a priority without looking at your data Things become different if you have one you test at 0.05 it's not significant and then you start eyeballing your data and you're saying I think the pattern looks a bit like this let's see if I can find the test analysis typical your analysis is not just one p-value there's a whole series of analogies and you try to build a consistent story and that also is useful for convincing reviewers don't don't don't don't try to shoot a review or a small p-value in his eye especially not a fabricated p-value don't do this questions.
# Transcript
![](https://www.youtube.com/watch?v=x0hR-VsHZj8)  

Lecture by Eric Maris during the "Advanced analysis and source modeling of EEG and MEG data" Toolkit of Cognitive Neuroscience at the Donders Institute.
# Others
