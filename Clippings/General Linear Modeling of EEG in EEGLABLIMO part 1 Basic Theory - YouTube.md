---
title: "General Linear Modeling of EEG in EEGLAB/LIMO part 1: Basic Theory - YouTube"
source: "https://www.youtube.com/watch?v=mZbK6KvMF2I&t=68s"
author:
published:
created: 2024-11-23
description: "Desfrute dos vídeos e da música que adora, carregue conteúdo original e partilhe-o com amigos, familiares e o mundo no YouTube."
tags:
  - "clippings"
---
#
# Notes
# Content 
# Transcript

This presentation is about [[General Linear Modeling]] in [[EEGLAB]]/LIMO. There are two parts to this presentation. The first one has to do with theory and the second one with practice.

Video title: General Linear Modeling of [[EEG]] in EEGLAB/LIMO part 1: Basic Theory
    
Video URL: https://www.youtube.com/watch?v=mZbK6KvMF2I&t=68s
    
Video language: English
    
--------------------------------

Hello, my name is Arnaud Delorme and this presentation is about General Linear Modeling in EEGLAB/LIMO. There are two parts to this presentation. The first one has to do with theory and the second one with practice. EEGLAB has been develop by ourselves at UCSD and LIMO has been developed by Cyril Pernet at University of Edinburg with input from us. LIMO is one of the most important plugins of EEGLAB. In this presentation, there are a lot of statistics. I want to mention though that I am not a statistician. This is GOOD because you might actually stand a chance to understand what I am talking about. However, although I am convinced that none of the concepts in this presentation are inaccurate, there might be simplifications which a statistician would judge outrageous. So bear that in mind. First, I am going to explain to you why we use the General Linear Model. The General Linear Model is a method that encompass most statistics you might already know, such as t-test, simple regression, multiple regression, ANOVA, ANCOVA, etc... But it also allows designing more complex model as I will explain here.  First a regression is a linear model. Imagine a psychophysics experiment where images with of different contrasts are presented, and we ask participants to press a button whenever they see an animal. This is actually an experiment we have done, and this is the histogram of reaction time for different levels of contrasts. Now, we want to model this behavior with a linear regression. Our model is that the reaction time is equal to a factor multiplied by the contrast in the image plus a constant and an error term. We will call this constant beta0 – which is the intercept -- and this factor beta1 – which is the slope. We can then do some simple calculation or use our preferred software to find the parameters beta0 and beta1 that best satisfy this formula. Here for example we obtain a beta0 of 2.7 and a beta 1 of 23.6. So we have built a simple linear model, which is a type of GLM. Here is another representation of this model. For each trial, we have a different error and we try to minimize this error. Now to test if the model is useful, we can compute significance. To do so, we compare the fit of this model – or the r square value - with the fit of a simpler model where we would not have the parameter beta1. We can also calculate the confidence interval of the beta1 parameter and assess if 0 is included in the 95% confidence interface of beta1. If 0 is not included, it means that beta1 is significantly different from 0. So this was a simple, if not the simplest, form of GLM. Now, an ANOVA is also a linear model.  Imagine an experiment where you would have 3 categories of images containing an animal. You would still respond only when the image contains an animal, but now we want to know if people respond faster on images of reptiles than on images of fishes or birds. For example, there might be an instinctive response to be faster to move when we see a snake or another reptile. To do so we would use this type of linear model, where we now have one beta parameter per category of animal. That is to say the data (for example reaction time) is equal to a constant term (the grand mean beta0) plus the effect of a treatment (beta1 for fishes 1 and beta2 for birds, and beta3 for reptiles) and the error term epsilon. For example, for trial 4, which would be the first presentation of fishes, we have reaction time is equal to beta 0 plus beta 2 plus the error term. For trial 13 which would be the second trial containing an image of birds, we have the same relationship with a different error term. Note that beta 2 is a number that cannot vary across trials, only the error varies across trials for a given category of image. We would have similar relationship for reaction time on images of fishes where we would use beta 1 and for reptiles where we would use beta 3. The ANOVA variables are categorical variables – that can only be 0 and 1, in this case encoding the type of animal - and the contrast variable in the previous slide was a continuous variable that could take any value. To assess if the model is significant, the fit of the model with the beta parameter for each category, the R square, is compared with the fit of the model without these beta parameters. If it is significantly different, the ANOVA is significant. Again this is a GLM that is strictly equivalent to performing an ANOVA. Now a GLM can actually model both a Regression and an ANOVA at the same time – which is an ANCOVA. Still using the same example, now we vary both the category of target image and the contrast. Again we measure reaction time. For example for a trial that contain a bird at a given contrast, we would have the beta parameters corresponding to the ANOVA as we saw in the previous slide plus the beta parameter – in this case beta 4 – corresponding to contrast. And you can see how easy it is to create more complex GLMs and add new variables both continuous for regression and categorical for ANOVAs. Now let’s see how we can easily visualize which beta parameter is associated with each trial. Imagine we have 12 trials and 4 groups of images – so Gp represent groups here. So we have 3 trials for images of group one, 3 trials for images of group two, etc… This is exactly the same as we have done with the images of different types for the ANOVA, except now there are 4 groups instead of 3. Also we have the results now in variable Y instead of using reaction time. We can represent these 12 equations using matrix notation, where we have a column vector representing the result which is equal to the design matrix containing 0 and 1 multiplied by a vector of beta parameters plus some error. The design matrix is often represented by this type of checkerboard where we encode 1s as a white cells and zeros as a black cells, and you often will see this type of representation in the literature. Now, let’s move to EEG and ERP processing.  Let’s first look at a single electrode, and focus on the first sample of multiple trials. Each trial has a corresponding row in the design matrix. Here we have 2 categories of stimuli and a continuous parameter which is the noise in the stimulus – for example, it can be a noisy image. You can see in the noise columns that we have grey level indicating that we have more than 0 and 1: we have a full gradient of noise values. To fit the GLM, we optimize beta parameters at each latency and do that iteratively for all latencies. So these are a lot of GLMs to fit. So now we have beta 1, beta 2, etc… across time and we can build ERP-like diagrams out of them, so we can assess how the EEG data respond to the presentation of different types of stimuli. For continuous variable, in this case the stimulus noise level, we have an image. In this case, trials have been sorted by the amount of noise so we can clearly see how the amount of noise affects single trial beta ERP. What is the difference between fitting a GLM on one hand and doing two separate statistical analyses: a regression plus an ANOVA of the EEG on the other hand. Looks like the same, right? Well, it is not. The advantage of doing both at the same time – when we fit a GLM - is that we regress out the contributions of other factors. Assuming the beta parameters corresponding to the noise models accurately model the response of the EEG signal to the noise content of the image, when the GLM fits beta parameters for the type of images, so the ANOVA part, it does that independently of the noise. It does it as if the noise had been removed. By contrast, if you fit a regression on one hand and an ANOVA on the other hand, the computation of beta for the ANOVA might be biased by the noise content in the stimuli. To assess if we have a significant difference between type of image, we can also subtract the beta parameters and assess if the 95% confidence interval of the difference overlap with ordinate 0. If it does, the beta is not significant. To calculate the 95% confidence interval of the difference, we could use parametric statistics, but we usually use bootstrap, where we repetitively take a subsample of trials and recompute all the models to determine how the beta parameters are affected. I put a link in the description on a lecture on confidence intervals, bootstrap and statistics. And of course, we have more than 1 electrode, so we have to repeat that for all electrodes. Then we can extract scalp topographies of beta parameters at latencies of interest, and also assess significance using the 95% confidence interval as I just mentioned. When you have only 2 types of stimuli, instead of scalp topographies of betas, we can also plot scalp topography of EEG potential difference between stimulus type. Then we can use the same mask for significance as we were using for the beta parameters. Again, if you just had 2 types of stimuli in your model, applying this mask would be equivalent to performing a t-test between your sets of ERPs for each type of stimulus, and thresholding the scalp topography at a p-value of 0.05. Note that we really fit a large number of models here. One model at each sample multiplied by the number of electrodes, so of course, we need to correct for multiple comparisons. There are multiple ways to correct for multiple comparisons, and I invite you again to look at the statistics lecture linked in the description for more information. I want to point on this slide that even for a given experiment, you can model your data in different ways. For example, say you have 4 types of visual stimuli. Red disk, green disk, red square, and green square. The default design matrix is to have one beta for each type of stimulus. This is called an interaction design and is shown here in section 1. With this design you can still compare between red and green stimuli by summing betas. For example, difference between red and green would be beta1 plus beta3 minus beta2 and Beta4. Difference between square and circle would be beta1 plus beta2 minus beta3 and Beta4. Alternatively, you can model it differently, with one beta for square, one beta for circle, one beta for green color, and one beta for red color. This is called a factorial design. Now square versus circle is beta 1 minus beta 2 and red versus green is beta 3 minus beta 4. This type of design is rarely used but it is still valid. A third design, which is called the full factorial design, will combine both. It will model both main effect of each stimulus characteristic and interaction between them. What do I mean by interaction? For example, it might be that shape has an effect on the EEG, that color has another additive effect, but that red square have a very special effect – there is a an interaction between the two types of characteristic (red and square) that leads a unique EEG characteristic. The full factorial model can be used when you want to study interactions at the single subject level. Otherwise, interaction are usually modelled at the second level, as we will see in a second. So I have shown you how to apply a GLM at the single subject level. We can even perform statistics across trials for single participants. Now, let’s move to group level analysis. Well, group level analysis is simple. It is the same as performing group level analysis of ERPs. However, instead of using average electrode time course for each subject as input, we use beta time course as input. Instead of using the grand average ERP for a given type of stimulus, we use the grand average beta parameter representing that type of stimulus. If you have a simple model with only different types of stimuli in your model, performing statistics on the beta will actually be equivalent as performing statistics on ERPs. Note that this is true only if you are using the ordinary least square method to optimize beta parameters. If you are using the alternative weighted least square method, then there is automatic weighting of individual trials to minimize artifacts, so we no longer have the equivalence with channel ERPs. Also, if you include continuous variables this will not be equivalent as I explained previously. So in this specific case, we can perform a two way repeated measure ANOVA – or the equivalent GLM design. The first main effect will indicate the influence of shape on the EEG data, the second main effect will indicate the influence of color, and the interaction terms possible interactions between shape and color. This is why you do not need to compute interactions at the first level. Instead you do it at the second level, as you would do with ERPs. Again this must be done at each latency for each electrode, so we will need to correct for multiple comparisons. I mentioned previously that you could also use a full factorial model at the first level. In this case, you have already extracted interactions at the first level so you cannot recompute them at the second level. If you have a full factorial model at the first level, you could use a 2 sample paired t-test between the beta for square and the beta for disks to assess the effect of shape. The effect of color would be a group 2 sample paired t-test between the beta for red and the beta for green. And for interaction effect, we can perform one sample t-test to assess if any of the interaction terms are different from 0 across subjects. This second model make sense if you want to study interactions at the single-subject level. LIMO and EEGLAB will use the model on the previous slide by default but they allow you to use that model as well. Also, I want to reiterate that this is a high-level explanation. There are a lot of details pertaining to the optimization of these models and the statistical assumptions for using them. For example, I mentioned on this slide and the previous one that we simply apply another GLM at the second level. However, at the second level, instead of a standard GLM, Cyril Pernet is using Hotelling statistical tests which do not need to be corrected for sphericity of the data and for potential correlations between measures. I invite you to look at the relevant publications in the link in the description.  Finally, and this is my last slide, why do a complex 2-level hierarchical GLM model while we could use a simpler mixed effect model where we model the different participants using additional beta parameters. This is the represented on the left here. You can see that now I have a design matrix that represents all the trials from all the participants, with 3 additional columns at the beginning to indicate which participants the trials belong to. If a trial belong to subject 1, I put “1” in the fist column of this design matrix. If a trial belong to subject 2, I put “1” in the second column of this design matrix, etc... This does seem much simpler and elegant and again it is perfectly fine to use this model. However, in practice, both when using LIMO and when using SPM for [[fMRI]], the two-level hierarchical approach is preferred. The reason is first computational. The model with all the participants is going to be very large and take a long time to process. Not all the data may fit in memory so these are additional practical considerations. In practice, Cyril Pernet has showed that both models return comparable results, so even though the mixed model effect is the more theoretically valid approach, it is fine to use the hierarchical model instead – and again, this is what people have been doing in fMRI for the past 25 years. I put a link to his paper in the description. I also want to point out that some researchers use GLM on EEG data in a different way. For example, some researchers have used beta parameters to model time. This is for example useful when you have stimuli that overlap in time. Or it might be possible to use other beta parameters to model electrodes, so it is not necessary to run a GLM for each time sample and for each electrode. Everything would already be included in the model, at the cost of having a model with a very very large number of beta parameters. This approach is not possible using LIMO, but it is good to know that it exists. So I hope you know a little bit more about GLM now. I want to thank you for your attention, and I hope to see you in one of my future videos.
