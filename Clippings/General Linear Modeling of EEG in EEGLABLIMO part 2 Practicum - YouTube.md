---
title: "General Linear Modeling of EEG in EEGLAB/LIMO part 2: Practicum - YouTube"
source: "https://www.youtube.com/watch?v=7EWN2w0kSy4"
author:
published:
created: 2024-11-23
description: "Desfrute dos vídeos e da música que adora, carregue conteúdo original e partilhe-o com amigos, familiares e o mundo no YouTube."
tags:
  - "clippings"
---
#
# Notes
# Content 
>[!figure] ![[Pasted image 20241123154519.png]]
>*Figure 1*: Code used for preprocessing in this video.

# Transcript
Video title: General Linear Modeling of EEG in EEGLAB/LIMO part 2: Practicum
    
Video URL: https://www.youtube.com/watch?v=7EWN2w0kSy4
    
Video language: English
    
--------------------------------

Hello, my name is Arnaud Delorme and this presentation is about General Linear Modeling in EEGLAB/LIMO. This is the second and last part of this presentation. The first one had to do with theory and this second one is about practice. EEGLAB has been developed by us at UCSD and LIMO has been developed by Cyril Pernet at University of Edinburg. LIMO is an extension of EEGLAB that is available in the EEGLAB plugin manager. There is a link in the description for how to install both EEGLAB and LIMO.  This presentation follows loosely a section of the LIMO tutorial, so I invite you to visit the tutorial for more information. I put the link in the description below. So first, let’s get some data. We will use data from a face presentation experiment that has already become popular. It was published a couple of years ago and it include, MEG, EEG, and fMRI – although not simultaneous EEG-fMRI. The original dataset is huge, so we have reformatted it to a BIDS dataset that only contains EEG data. BIDS is the Brain Imaging Data Structure and it is a way to format your raw data so it contains all information for other researchers to use. There is a video about BIDS and EEGLAB in the description below, so I am not going to enter the details of BIDS formatting. In the original data, there were multiple runs for each subject, so we have merged them. We have also formatted scanned electrode positions so they are available within the BIDS dataset. We have corrected event latencies and renamed some events. And we have resampled the data so it is not as large, and can be used for tutorials. So, it is the raw data with some modifications, and all modifications are all documented in the readme file. The BIDS dataset also contains the EEGLAB scripts used to convert the original data release with this modified one. So I invite you to download this dataset on OpenNeuro.org. It is about 4 Gigabytes.  I am going to describe the experiment briefly. There are 3 types of stimuli: Famous faces, Non-famous faces, and Scrambled faces. There are also 3 levels of repetition: 1st time presentation of a face, 2nd time (right after), 3rd time (many trials after). Participants pressed a key based on how symmetric they thought each image was. The reason for using this task was because it can be performed equally well on face and non-face stimuli. This accessory task also forces participants to keep attending to the images presented to them. Here, we need the conditions computed per subject at the 1st level – so there are 9 conditions total (3 types of faces by 3 presentation orders). For this simple tutorial, we are going to simply compare the EEG response to the type of face, but we could also perform repeated measure ANOVA to test main effects for face and presentation order as well as interactions between them. I linked the LIMO tutorial in the description if you are interested in these more advanced analyses.  So, let’s get started. First make sure you have the right version of EEGLAB and of LIMO installed in your environment. There is a link in the description. Then download the data. After you have downloaded the data, this is how the folder for all participants look like. Then open Matlab and start EEGLAB.  This is the preprocessing scripts with the different steps needed to preprocess the data. There is a video on preprocessing EEG data in BIDS, so I am not going to repeat it here. I put the link in the description below. Briefly, first we import the data, second we clean it lightly. Third we compute average reference – while temporarily interpolating bad channels which have been removed. Fourth, we run ICA and remove bad components. Fifth we clean the data more aggressively. This is slightly different from the other pipeline I have presented in the other BIDS video I was referring to. In the other pipeline, we cleaned the data only once before running ICA and also ICA components are subtracted at the STUDY level, not at the dataset level. Here we do it in two steps because there are a lot of eye movements and if we clean the data aggressively before ICA, we are going to remove all the portions of data containing eye artifacts. Instead we clean lightly to only remove portions of data containing very large artifacts, we run ICA and subtract artifact components including eye artifact components. Then we clean this resulting data more aggressively to remove smaller artifacts still present in the data. Finally we extract data epochs and create an EEGLAB study. So this is the script, but you can actually do it easily from the graphic interface as well. And as you do it in the graphic interface, the script gets generated for you. So I will do it quickly in the graphic interface. Again, for more details, I invite you to look at the video for going from raw data to group level analysis which I linked in the description. So here we go! And first we're going to do is to import the BIDS dataset, so that's the folder downloaded from Openeuro. There are many options here and i'm not going to describe them. I refer you to the other video for that. We have imported the data, then we're going to remove bad channels so these are channel 61 to 64. It depends on your study. Once we have removed the channels, we're going to perform artifact rejection using ASR. So that's the first one [rejection], that's the light one. So we only first select the first two steps in this case. Here we apply it and once it's done, we're going to perform average reference. So we reference the data and we added interpolated electrodes which had been removed and then remove them again [after interpolation]. So now we have referenced the data then we decomposed the data using ICA. So, here we have to enter in the options PCA minus one so we remove one dimension. Because of the average reference process, it should detect automatically the [decreased] rank but just in case. Then we label the bad components using IClabels and we also have to set thresholds for labeling components. Again everything is described in the other tutorial video. We remove these components from the data -- so here it asks us to confirm and once we do that, we can clean the data a little bit more aggressively. So we do the last two steps of ASR as shown here. Once we've done that, then we're ready to extract epochs so these are the types of epochs. These are all the nine types of stimuli: the famous pictures the familiar ones and the scrambled pictures, presentation one, two and three. So now, we have preprocessed the data. What is the next step?  The next step is to create a STUDY design that may be used both by LIMO and by EEGLAB native statistics functions. So here we will simply select the 3 types of stimuli: Famous faces, Non-famous faces, and Scrambled faces. So, again we will do that interactively in EEGLAB. So first we're going to create a study design. We're going to select the different types of images here famous familiar and unfamiliar and scrambled, so that's done and now we're going to rename the design. We're going to call it "stimuli" because we're comparing different types of stimuli. This is optional but it's good to have a name you can recognize. The next step is to have EEGLAB precompute measures. Note that this can be done before or after creating a design. Precomputing measures, such as single trial ERP, spectrum, and ERSP is a pre-requisite before running LIMO. Note that for ERP, since we already have single trial data, there is usually no need to recompute anything – unless you choose to change the baseline. But, to be consistent with other measures, like spectrum and ERSP, we still need to go through this step. Here we do not need to remove components labeled for rejection since we have already removed them. We should interpolate channels which were flagged as artifactual in some participants. So I am going to go through these steps. Checking the ERP [checkbox], selecting the baseline, and the n pressing ok to perform the precomputation.  And so once it's pre-computed we can use the standard EEGLAB statistics just to look at the the data, in particular channel 65 which was the channel used in the original study. So this is how channel 65 looks like. And i'm going to overlay the three conditions on top of each other so we can see the difference. So here just change the plotting options and here we go. So let's compare with this with the original one from the the paper. You can see that the waveform are quite similar. The time window in EEGLAB is larger and so we can see that the ERP seems to be riding on a very slow oscillation around 1 Hz. I am not sure it is a real feature of the data, but it might be worth looking into. In both cases, we can see that the N170 peak is large for faces compared to scrambled faces which is consistent with the literature. We can also see how the scrambled images tend to lead to ERP more positive after 300 ms when compared to the face conditions. We can also plot the scalp topographies. So I'm going to select all the channels and then I am going to go again in the parameters. I'm going to plot the scalp topographies between 160 and 180 milliseconds. And so here, it's going to plot the topography for the three conditions famous, scrambled and unfamiliar. And i'm going to compute statistics, just standard statistics, and use FDR for correction for multiple comparisons. There's a lecture on statistics if you want to understand exactly what it does. And here we can see clearly the significant regions -- so that's an ANOVA across the three conditions using EEGLAB standard statistics.  And you can also go on the command line and type "eegh" to see all the commands you've used and copy that into your script and rerun it as if you had done it from the interface. Now we're going to use LIMO to first estimate model parameters and then look at the GLM variables. And so we have 3 factors here plus the constant. It is important to remember the order of these factors since in LIMO, these are usually designated as 1, 2 and 3. So let’s remember that 2 are the scrambled images. In general, we would want to list all the possible factors – so the 9 conditions we extracted for each dataset with each image type and presentation order. Then we can group factors at the second level if we need to. For example, we can groups all the beta for faces compared to all the beta for scrambles images. Because this is a tutorial, and probably the first time you are using LIMO, we are going to make it simpler here, and just look at the type of images. But just know, that in general, it is better to use all the factors or variables available at the first level. So we're going to change here the time limit and we're going to put minus 50 to 600 milliseconds. It's just so that it calculates faster. We don't have to calculate every single sample so we can just select a specific time window. We press ok. You have to wait about like 10 minutes here for everything to be computed and once this  is done you can go to second level if analysis. And first you have to load the channel locations that are automatically generated. And here then you can compute your ANOVA. And so full repeated measure anova, full scalp analysis, and a bunch of questions: how many groups? Just one group, one group of subject here. What's the repeated factor level and we have three. We want to use betas and parameters so we have to select the file containing all the beta parameters which is in the LIMO folder. I'm showing the file here. It contains all the file for every single subject. So that's what this text file contains. Then we select which beta parameters with four of them: one, two, three plus the constant. We only want one two, and three so we select these three beta parameters and then we can give a name to this analysis so we can recognize it later. It asks us if this is the design matrix for the ANOVA. Are you happy with this? Yes! And it's computing including bootstrap analysis. This takes another 10 minutes and once this is done we can start looking at the results. That's the next menu on the list: LIMO results. And we're going to click "image all" and here select the file that was generated for the repeated measure ANOVA. And so it's going to show us all the electrodes and the scalp topography. And we can also use clustering -- so that's a method for correcting for multiple comparison. There is a lecture on statistics containing details about this method. So that's now corrected for multiple comparison. We can also click on specific parts of the image at specific latencies to see the contrast difference. So here you see a scalp topography that looks very much like the one we had in EEGLAB. It makes sense because it's a very similar analysis.  Now we're going to look at contrasts. So to look at contrast we have to first load a LIMO matrix. So now we have loaded a limo matrix. And at this stage, the ANOVA results tell us where and when those three conditions differ. To check which condition differs from the other, post-hoc contrasts can be performed testing between pairs of conditions. I am showing the different contrasts on the screen: we have familiar faces vs. scrambled faces which would be beta1 minus beta2, unfamiliar faces vs scrambled faces which would be beta3 minus beta2 and familiar faces vs unfamiliar faces which would be beta1 minus beta3. Note that these analyses differ from performing ad-hoc t-tests, mostly because they are computed within the repeated ANOVA model, which accounts for all conditions and also they are not directional because they rely on a F statistic. We can also have more complex contrasts. For example, faces vs scrambled faces when we have the average of beta1 and beta3 compared to beta2. This compares images containing faces with images not containing faces. Note, even though it seems like a hack to use these contrast matrices, this is exactly what statistical software do when performing post-hoc analyses. So let’s try this last one. Here we're going to enter the the contrast and just press done. It's asking if we want to compute bootstrap. Yes! So we're computing all the bootstrap and now we're going to image these results. So that's the file that's named ESS. You might have to go to the parent folder. And now that's the comparison between faces and not faces. And we can also look at a given channel, so here we are plotting the channel with the most difference. That would be channel 2 along with the 95 confidence interval and the region of significance on the bottom. Here we can also correct for multiple comparison using clustering again. We can "image all" and also look specifically at a latency of interest. So now this is the comparison between faces and non-faces and this is very similar to what they found in the original article.  So this is the end of this presentation, and of the introduction to LIMO. The LIMO tutorial contains many more examples using this dataset and I encourage you look at it. I want to thank you for your attention, and I hope to hopefully see you in one of my future videos.
