---
citekey: "cohenRigorReplicationTimefrequency2017"
title: "Rigor and replication in time-frequency analyses of cognitive electrophysiology data"
itemType: "journalArticle"
publicationTitle: "International Journal of Psychophysiology: Official Journal of the International Organization of Psychophysiology"
bookTitle: ""
seriesTitle: ""
publisher: ""
place: ""
volume: "111"
numberOfVolumes: ""
issue: ""
pages: "80-87"
edition: ""
date: "2017-01-01"
DOI: "10.1016/j.ijpsycho.2016.02.001"
ISBN: ""
ISSN: "1872-7697"
url: ""
importance: 
status: incomplete
tags:
  - article
---

## Rigor and replication in time-frequency analyses of cognitive electrophysiology data

### Table of Contents

- [Annotations](#annotations)

+ [Commentaries](#commentaries)

- [Appendix](#appendix)

### Annotations




#### Page 3







> Simultaneously, many areas of modern sciences are experiencing a “replication crisis,” prompting discussions of best practices to produce robust and replicable research.





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 2:31 am

---



#### Page 4







> On the other hand, diversity and variability are sources of frustration in science, and require scientists to awkwardly straddle ecological validity (poorly controlled and poorly measured diversity) and experimental control (overly constrained environments that might not reflect natural behavior).





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:27 am

---







> To make matters worse, we might not recognize the Truth even if we happen to stumble upon it. Therefore, the best we can strive for is “Consistency.”





- **Color**: #2ea8e5 (Blue)
- **Date**: 2024-10-26 3:26 am

---







> Needless to say, we all want to do replicable research. No one actually wants to publish findings that cannot be replicated. This is the primary motivation for collecting data from N>1 subjects.




**Comment**: Unless that research applies to the self. Then it has a lot of value.


- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:00 am

---



#### Page 5







> Scientists have been known to switch fields because they get bored with replicating their own findings.





- **Color**: #2ea8e5 (Blue)
- **Date**: 2024-10-26 3:01 am

---







> Unlike questionnaire-based or simple computer-based tasks, an EEG study focusing on time-frequency-based analyses might take 1-2 years to complete, and it might require several years of training before being able to analyze the dataset appropriately.





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:01 am

---








> [[time-frequency analyses]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:02 am

---








> [[neural oscillations]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:02 am

---








> [[EEG]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:02 am

---








> [[MRI]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:03 am

---







> Furthermore, despite the huge differences in the sizes of the brain over different species, the speeds of neural oscillations have remained remarkably constant (Buzsáki et al. 2013).





- **Color**: #e56eee (Magenta)
- **Date**: 2024-10-26 3:03 am

---







> Time-frequency-based analyses are the best approach to allow inferences regarding neural oscillations.





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:03 am

---



#### Page 6







> Oscillations as a mechanism for controlling the flow of information in the brain is faster and less permanent than synaptic plasticity or other structural changes associated with long-term learning.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:13 am

---








> [[synaptic plasticity]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:13 am

---







> neural oscillations are thought to provide an internal clocking mechanism for coordinating neural computations (Buzsáki and Moser 2013).





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:13 am

---



#### Page 8








> [[functional connectivity]], [[cross-frequency coupling]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:17 am

---








> [[spatial multivariate analyses]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:17 am

---







> Design your experiments with replications in mind.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:19 am

---







> Second, whenever possible, try to incorporate replications of existing findings into new experiments.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:18 am

---







> For ERPs, baselining simply involves a linear subtraction of the pre-trial (or sometimes pre-response) average voltage value. For time-frequency power, however, a modification of this procedure is required for two reasons.





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:19 am

---







> First, power (the amount of energy in a signal at each frequency) generally follows a “1/f” pattern, meaning that power decreases with increasing frequencies, typically in a supralogarithmic manner. Comparing power across frequencies therefore requires a nonlinear normalization—typically, decibel or percent change.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:19 am

---







> Second, because of temporal smoothing inherent in time-frequency analysis methods, a baseline period that ends at time=0 is suboptimal because early post-stimulus activity may “leak” into the estimate of the baseline activity.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:21 am

---



#### Page 9







> In most situations, a pre-trial baseline period of -500 to -200 ms is sufficient. The choice of baseline should be considered during experiment design.




**Comment**: This is contrary to ERP studies in which a baseline period from -200 to 0 is more suitable (see Luck,2014)


- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:22 am

---







> Have “enough” trials per condition and “enough” subjects. Unfortunately, it is difficult to give precise numbers that would be appropriate for all experiments, because it depends on the effect size and the quality of the data. As a general guideline, I recommend a minimum of 50 trials per condition per subject (ideally >100), and at least 20 subjects.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:23 am

---







> When possible, trial counts should be matched across experiment conditions. Trial count differences can lead to condition differences in signal-to-noise characteristics





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:26 am

---







> or some analyses including phase-based analyses, unbalanced trial counts can bias the results towards the conditions with fewer trials.





- **Color**: #e56eee (Magenta)
- **Date**: 2024-10-26 3:26 am

---







> Low frequencies, and analysis parameters that involve more smoothing, generally produce higher signal-to-noise results and therefore may require fewer trials.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:32 am

---







> Previous studies with similar experiment design and data analyses can be a guide, but piloting is nearly always the best way to estimate effect sizes.





- **Color**: #2ea8e5 (Blue)
- **Date**: 2024-10-26 3:33 am

---







> In the pilot experiment, a few motivated subjects can perform the experiment using as many trials as they can tolerate. In the analyses, the reliability of the effects can be assessed using random subsets of trials. The minimum number of trials that reliably reproduces the effects observed with all of the trials can be taken as the minimum number of trials to produce a stable effect.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:33 am

---







> When data are acquired from living animals, be aware that there is a balance between a high trial count and engagement in the task. After too many trials, subjects may become disengaged or tired, and this may result in noisy data that only decreases statistical robustness and replicability. This is a greater danger in some populations such as children or patients.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:34 am

---







> Despite some well-intentioned efforts to automate artifact rejection, I will argue below that the best artifact rejection procedure is manual, i.e., human-based and visually guided, and therefore unfortunately open to subjectivity.





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:34 am

---



#### Page 10







> The problem is that I and many other M/EEG researchers with whom I have spoken agree that automatic algorithms produce both Type-I and Type-II errors.





- **Color**: #e56eee (Magenta)
- **Date**: 2024-10-26 3:35 am

---







> Unless the noise is extreme, I recommend leaning on the side of not removing components. Thinking that EEG data can or should be noise-free is dangerous and incorrect, and it's better to leave some noise in than take signal out (extreme cases notwithstanding).





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:36 am

---







> When in doubt, ask a colleague to look at the component.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:36 am

---







> I encourage people not to clean more than 2-3 datasets per day. Exhaustion and frustration can change the criteria. One way to assess the subjectivity is to have two independent raters perform manual trial/ICA rejection on the same datasets, and measure inter-rater-reliability.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:37 am

---







> One should not think “this trial is from condition 'A' and so therefore I should/shouldn't reject it.” Cleaning the data prior to separating the epochs into each condition is a good way to ensure condition-blindness. If the experiment involves different groups (e.g., patients and controls), the person cleaning the data should see only arbitrary dataset identifiers and not group membership. Ideally, only one person should clean the entire dataset.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:37 am

---



#### Page 11








> [[github]], [[google-code]], [[figshare]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:38 am

---







> Statistical analyses can be done at two different “levels,” where “level” refers to what is considered the unit of data for statistical analysis. Within- subjects statistics (also called level-1) consider the trial to be the unit for analysis, while group-level statistics (level-2) consider trial-averaged data per subject to be the unit for analysis.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:40 am

---







> Within-subjects analyses provide information regarding the cross-trial variability of an effect relative to the magnitude of the effect; they provide no information regarding the generalizability of the effect to other subjects. Group-level analyses, on the other hand, provide information regarding the consistency of the direction of the effect across the group of subjects, and provide little information regarding the within-subject variability across trials.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:40 am

---



#### Page 12








> [[split-half replication]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:40 am

---



#### Page 13




![](<0 - Supplementary/images/cohenRigorReplicationTimefrequency2017.md/image-13-x52-y57.png>)



> *(No annotated text)*




- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:44 am

---



#### Page 14







> There is a strong—and typically unstated—assumption in group-level analyses, which is that there is no meaningful variability in temporal-spatial-spectral localization across subjects. That is, group-level analyses assume that, for example, activity from 8-10 Hz and 200-500 ms at electrode Pz reflects the same process for all subjects.





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:45 am

---







> This is a questionable assumption. For example, there are considerable individual differences in peak frequency (Haegens et al. 2014), which are related to a combination of genetics (Posthuma et al. 2001), neurochemical factors (Muthukumaraswamy et al. 2009, but see Cousijn et al. 2014 for a nonreplication), age (Polich 1997) and so on.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 3:45 am

---







> There are two alternative approaches that may increase statistical sensitivity by incorporating individual variability in brain functional and structural anatomy. One approach is to select electrodes and/or time-frequency windows for each subject.





- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 3:58 am

---








> [[circular inferences]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:46 am

---








> [[joint decorrelation]], [[generalized eigenvalue decomposition]], [[principle components analysis]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 3:46 am

---







> Furthermore, analysis code should always be made available so non-experts can evaluate and use the new methods (this seems like too obvious a point to mention, but many methods papers report only equations and do not provide implementations).





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 3:48 am

---



#### Page 15







> Sparsity of results make papers more easily criticized and more easily forgotten, and therefore less likely to be cited.





- **Color**: #2ea8e5 (Blue)
- **Date**: 2024-10-26 4:00 am

---



#### Page 16




![](<0 - Supplementary/images/cohenRigorReplicationTimefrequency2017.md/image-16-x53-y114.png>)



> *(No annotated text)*




- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-26 4:01 am

---



#### Page 17








> [[correlation]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 4:03 am

---








> [[ANOVA]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 4:03 am

---








> [[factor analysis]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 4:03 am

---







> Ambiguous terms like “synchronization” and “time-frequency response,” if used, should be clearly and carefully defined in the Methods section.




**Comment**: A glossary is another solution for this problem


- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-26 4:04 am

---







> Preregistration is a model for scientific publishing in which authors essentially have the Methods section peer-reviewed prior to starting the study, and once approved by reviewers and editors, the manuscript is in-principle accepted provided the experiment is conducted along the lines of the approved manuscript.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-26 4:05 am

---



#### Page 18








> [[Cortex]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 4:05 am

---








> [[arXiv.org]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 4:20 am

---








> [[bioRxiv.org]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-26 4:20 am

---



#### Page 19







> Cognitive electrophysiology has a “wild-west” feel to it, and this should be embraced rather than discouraged.





- **Color**: #2ea8e5 (Blue)
- **Date**: 2024-10-26 4:22 am

---





### Notes


No notes available.


%% begin notes %%

### Bookmark

- **Bookmark**: Page <!-- Specify the page number or section -->
- **Status**: #complete 
- **Relevance**: 8.5
## Commentaries



%% end notes %%

## Appendix

### Authors


- [[Mike X. Cohen]] (author)



### Abstract

Cognitive electrophysiology is a subfield of neuroscience that focused on linking M/EEG data to aspects of cognition and the neurophysiological processes that produce them. This field is growing in terms of the novelty and sophistication of findings, data, and data analysis methods. Simultaneously, many areas of modern sciences are experiencing a "replication crisis," prompting discussions of best practices to produce robust and replicable research. The purpose of this paper is to contribute to this discussion with a particular focus on cognitive electrophysiology. More issues are raised than are answered. Several recommendations are made, including (1) incorporate replications into new experiments, (2) write clear Methods and Results sections, and (3) publish null results.


### Formatted Bibliography

Cohen, M. X. (2017). Rigor and replication in time-frequency analyses of cognitive electrophysiology data. _International Journal of Psychophysiology: Official Journal of the International Organization of Psychophysiology_, _111_, 80–87. [https://doi.org/10.1016/j.ijpsycho.2016.02.001](https://doi.org/10.1016/j.ijpsycho.2016.02.001)


### Tags


- #electroencephalography

- #eeg

- #replication

- #humans

- #magnetoencephalography

- #cognition

- #electrophysiology

- #data_interpretation,_statistical

- #methods

- #oscillations

- #reproducibility_of_results

- #time-frequency




### Attachments


- **Cohen - 2017 - Rigor and replication in time-frequency analyses of cognitive electrophysiology data.pdf**: C:\Users\joaop\Zotero\storage\WNW8UQ3H\Cohen - 2017 - Rigor and replication in time-frequency analyses of cognitive electrophysiology data.pdf

- **PubMed entry**: 




### Collections


- standards





### Backlinking


#### Metadata Links


- publicationTitle: [[International Journal of Psychophysiology: Official Journal of the International Organization of Psychophysiology]]




- date: [[2017]]






%% Import Date: 2025-01-09T18:07:42.405+00:00 %%
