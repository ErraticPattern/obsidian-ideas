---
citekey: "mirandaCombinedModelfreeModelsensitive2020"
title: "Combined model-free and model-sensitive reinforcement learning in non-human primates"
itemType: "journalArticle"
publicationTitle: "PLOS Computational Biology"
bookTitle: ""
seriesTitle: ""
publisher: ""
place: ""
volume: "16"
numberOfVolumes: ""
issue: "6"
pages: "e1007944"
edition: ""
date: "2020-06-22"
DOI: "10.1371/journal.pcbi.1007944"
ISBN: ""
ISSN: "1553-7358"
url: "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007944"
importance: 
status: incomplete
tags:
  - article
---

## Combined model-free and model-sensitive reinforcement learning in non-human primates

### Table of Contents

- [Annotations](#annotations)

+ [Commentaries](#commentaries)

- [Appendix](#appendix)

### Annotations




#### Page 2








> [[Reinforcement learning]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---








> [[model-based]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---








> [[model-free]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---








> [[Tolmanâ€™s cognitive maps]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---







> Their estimates of long-run rewards are thereby readily adaptive to environmental or motivational changes that are known to the model, just like goaldirected actions





- **Color**: #ffd400 (Yellow)
- **Date**: 2024-10-25 8:47 pm

---







> MF-RL typically requires substantial sampling from the world to achieve good performance, and is therefore, like behavioural habits, slow to adapt to environmental change.





- **Color**: #ff6666 (Red)
- **Date**: 2024-10-25 8:47 pm

---








> [[model-sensitive]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---







> we regard as MS any dependencies that are associated with the structure of the task rather than purely previous rewards.





- **Color**: #e56eee (Magenta)
- **Date**: 2024-10-25 8:47 pm

---



#### Page 3




![](<0 - Supplementary/images/mirandaCombinedModelfreeModelsensitive2020.md/image-3-x40-y93.png>)



> *(No annotated text)*




- **Color**: #a28ae5 (Purple)
- **Date**: 2024-10-25 8:47 pm

---



#### Page 12








> [[two-stage Markov decision task]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---



#### Page 19








> [[Bayesian information criterion]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---








> [[likelihood-ratio tests]]





- **Color**: #5fb236 (Green)
- **Date**: 2024-10-25 8:47 pm

---





### Notes


No notes available.


%% begin notes %%

### Bookmark

- **Bookmark**: Page <!-- Specify the page number or section -->
- **Status**: #incomplete
- **Relevance**: 8.5
## Commentaries



%% end notes %%

## Appendix

### Authors


- [[Bruno Miranda]] (author)

- [[W. M. Nishantha Malalasekera]] (author)

- [[Timothy E. Behrens]] (author)

- [[Peter Dayan]] (author)

- [[Steven W. Kennerley]] (author)



### Abstract

Contemporary reinforcement learning (RL) theory suggests that potential choices can be evaluated by strategies that may or may not be sensitive to the computational structure of tasks. A paradigmatic model-free (MF) strategy simply repeats actions that have been rewarded in the past; by contrast, model-sensitive (MS) strategies exploit richer information associated with knowledge of task dynamics. MF and MS strategies should typically be combined, because they have complementary statistical and computational strengths; however, this tradeoff between MF/MS RL has mostly only been demonstrated in humans, often with only modest numbers of trials. We trained rhesus monkeys to perform a two-stage decision task designed to elicit and discriminate the use of MF and MS methods. A descriptive analysis of choice behaviour revealed directly that the structure of the task (of MS importance) and the reward history (of MF and MS importance) significantly influenced both choice and response vigour. A detailed, trial-by-trial computational analysis confirmed that choices were made according to a combination of strategies, with a dominant influence of a particular form of model sensitivity that persisted over weeks of testing. The residuals from this model necessitated development of a new combined RL model which incorporates a particular credit assignment weighting procedure. Finally, response vigor exhibited a subtly different collection of MF and MS influences. These results provide new illumination onto RL behavioural processes in non-human primates.


### Formatted Bibliography

Miranda, B., Malalasekera, W. M. N., Behrens, T. E., Dayan, P., & Kennerley, S. W. (2020). Combined model-free and model-sensitive reinforcement learning in non-human primates. _PLOS Computational Biology_, _16_(6), e1007944. [https://doi.org/10.1371/journal.pcbi.1007944](https://doi.org/10.1371/journal.pcbi.1007944)


### Tags


- #animal_behavior

- #decision_making

- #eyes

- #learning

- #linear_regression_analysis

- #primates

- #reaction_time

- #simulation_and_modeling




### Attachments


- **Full Text PDF**: C:\Users\joaop\Zotero\storage\XYQNDTQB\Miranda et al. - 2020 - Combined model-free and model-sensitive reinforcement learning in non-human primates.pdf




### Collections


- rl





### Backlinking


#### Metadata Links


- publicationTitle: [[PLOS Computational Biology]]




- date: [[2020]]






%% Import Date: 2025-01-09T18:03:54.453+00:00 %%
